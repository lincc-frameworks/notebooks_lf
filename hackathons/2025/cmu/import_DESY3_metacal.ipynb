{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6842fbb0-d241-45f9-93cf-4c58f02c1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -r requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7bdb015-2993-4a0c-8856-3ce79757f2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/delucchi/.conda/envs/hipscatenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.6.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hats\n",
    "from hats_import.catalog.file_readers import InputReader\n",
    "import pyarrow as pa\n",
    "import h5py\n",
    "import numpy as np\n",
    "from hats_import.catalog.arguments import ImportArguments\n",
    "\n",
    "\n",
    "hats.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815ef432-f594-40cc-a4a2-5a2645bcd43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeCatalogReader(InputReader):\n",
    "    def __init__(self, input_path):\n",
    "        self.input_path = input_path\n",
    "        self.chunksize=100_000\n",
    "        \n",
    "    def read(self, input_file, read_columns=None):\n",
    "        fh = h5py.File(self.input_path,'r')\n",
    "        dataset_len = fh['catalog']['unsheared']['ra'].len()\n",
    "\n",
    "        ## input_file will be formatted like \"interval_start:interval_end\"\n",
    "        interval_pieces = input_file.split(':')\n",
    "        (chunk_start, interval_end) = int(interval_pieces[0]), int(interval_pieces[1])\n",
    "        interval_end = min(interval_end, dataset_len)\n",
    "        chunk_end = chunk_start + self.chunksize\n",
    "\n",
    "        col_names = []\n",
    "        if read_columns is None:\n",
    "            tab_names = [f\"{key}\" for key in fh['catalog']['unsheared'].keys()]\n",
    "            col_names.extend(tab_names)\n",
    "            for table in ['sheared_1m', 'sheared_1p', 'sheared_2m', 'sheared_2p']:\n",
    "                tab_names = [f\"{table}_{key}\" for key in fh['catalog'][table].keys()]\n",
    "                col_names.extend(tab_names)\n",
    "\n",
    "        while chunk_start < interval_end:\n",
    "            if read_columns is None:\n",
    "                col_vals = []\n",
    "                tab_values = [fh['catalog']['unsheared'][key][chunk_start:chunk_end] for key in fh['catalog']['unsheared'].keys()]\n",
    "                tab_values = [np.asanyarray(arr, dtype=arr.dtype.newbyteorder(\"=\")) for arr in tab_values]\n",
    "    \n",
    "                col_vals.extend(tab_values)\n",
    "                for table in ['sheared_1m', 'sheared_1p', 'sheared_2m', 'sheared_2p']:\n",
    "                    tab_values = [fh['catalog'][table][key][chunk_start:chunk_end] for key in fh['catalog'][table].keys()]\n",
    "                    tab_values = [np.asanyarray(arr, dtype=arr.dtype.newbyteorder(\"=\")) for arr in tab_values]\n",
    "        \n",
    "                    col_vals.extend(tab_values)\n",
    "                yield pa.Table.from_arrays(col_vals, names=col_names)\n",
    "            else:\n",
    "                ras = fh['catalog']['unsheared']['ra'][chunk_start:chunk_end]\n",
    "                ras = np.asanyarray(ras, dtype=ras.dtype.newbyteorder(\"=\"))\n",
    "                decs = fh['catalog']['unsheared']['dec'][chunk_start:chunk_end]\n",
    "                decs = np.asanyarray(decs, dtype=decs.dtype.newbyteorder(\"=\"))\n",
    "                yield pa.Table.from_arrays([ras, decs], names=[\"ra\", \"dec\"])\n",
    "            \n",
    "            chunk_start += self.chunksize\n",
    "            chunk_end = min(chunk_end+self.chunksize, dataset_len, interval_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72560161-ed62-4df5-9813-ab21730f7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/ocean/projects/phy210048p/shared/hats/raw/DESY3_metacal_v03-004.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfbef59c-5c88-4d17-8054-d16481f84121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk = 1_000_000\n",
    "\n",
    "read_keys = [f\"{interval}:{interval+chunk}\" for interval in range(0, 400_000_000, chunk)]\n",
    "len(read_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f48fb9-529f-490f-8f10-ab22083cf4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ImportArguments(\n",
    "    sort_columns=\"coadd_object_id\",\n",
    "    ra_column=\"ra\",\n",
    "    dec_column=\"dec\",\n",
    "    input_file_list=read_keys,\n",
    "    file_reader=ShapeCatalogReader(path),\n",
    "    expected_total_rows=399263026,\n",
    "    output_artifact_name=\"DESY3_metacal\",\n",
    "    output_path=\"/ocean/projects/phy210048p/shared/hats/catalogs/des/\",\n",
    "    resume=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1fb20b4-9c23-48cb-8d05-bf649860c857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Planning  : 100%|██████████| 4/4 [00:00<00:00, 781.75it/s]\n",
      "Mapping   : 100%|██████████| 400/400 [00:29<00:00, 13.46it/s]\n",
      "Binning   : 100%|██████████| 2/2 [00:57<00:00, 28.54s/it]\n",
      "Splitting : 100%|██████████| 400/400 [17:03<00:00,  2.56s/it]  \n",
      "Reducing  : 100%|██████████| 1051/1051 [09:07<00:00,  1.92it/s] \n",
      "Finishing : 100%|██████████| 5/5 [25:25<00:00, 305.04s/it]   \n",
      "2025-09-04 10:35:38,091 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,189 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,189 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,190 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,191 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,191 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,192 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,193 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,194 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,195 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,195 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,196 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,197 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,198 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,199 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,199 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,200 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,201 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,202 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,202 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,203 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,204 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,205 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,206 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,206 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,207 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,208 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,209 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,210 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-09-04 10:35:38,210 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n"
     ]
    }
   ],
   "source": [
    "from hats_import import pipeline_with_client\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "\n",
    "local_tmp = os.path.expandvars(\"$LOCAL\")\n",
    "\n",
    "with Client(\n",
    "        local_directory=local_tmp,\n",
    "        n_workers=30,\n",
    "        threads_per_worker=1,\n",
    "        memory_limit=None,\n",
    "    ) as client:\n",
    "        pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebcba667-a38e-489b-a77d-3a27039798b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and schema.\n",
      "\n",
      "Starting: Test hats.io.validation.is_valid_catalog.\n",
      "Validating catalog at path /ocean/projects/phy210048p/shared/hats/catalogs/des/DESY3_metacal ... \n",
      "Found 1051 partitions.\n",
      "Approximate coverage is 20.05 % of the sky.\n",
      "Result: PASSED\n",
      "\n",
      "Starting: Test that files in _metadata match the data files on disk.\n",
      "Result: PASSED\n",
      "\n",
      "Starting: Test that number of rows are equal.\n",
      "\tfile footers vs catalog properties\n",
      "\tfile footers vs _metadata\n",
      "Result: PASSED\n",
      "\n",
      "Starting: Test that schemas are equal, excluding metadata.\n",
      "\t_common_metadata vs truth\n",
      "\t_metadata vs truth\n",
      "\tfile footers vs truth\n",
      "Result: PASSED\n",
      "\n",
      "Verifier results written to verification/DESY3_metacal/verifier_results.csv\n",
      "Elapsed time (seconds): 14.19\n"
     ]
    }
   ],
   "source": [
    "from hats_import import pipeline, VerificationArguments\n",
    "args = VerificationArguments(\n",
    "    input_catalog_path=\"/ocean/projects/phy210048p/shared/hats/catalogs/des/DESY3_metacal\",\n",
    "    output_path=\"./verification/DESY3_metacal\",\n",
    ")\n",
    "pipeline(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be683a35-3d2f-4be8-81dd-24d6c09fa5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hipscat_mmd11",
   "language": "python",
   "name": "hipscat_mmd11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
