{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6171e5bbd47ce869",
   "metadata": {},
   "source": [
    "# Run a Periodogram Across Full ZTF Sources\n",
    "\n",
    "This notebook is an adaptation of the Nested Dask [tutorial for loading HiPSCat data](https://nested-dask.readthedocs.io/en/latest/tutorials/work_with_lsdb.html).\n",
    "\n",
    "There we loaded a small subset of ZTF DR 14, and here we will try to load a full ZTF DR 20 and run a `light-curve` package periodogram across all of ZTF on epyc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a18789",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c055a44b8ce3b34",
   "metadata": {},
   "source": [
    "## Install dependencies for the notebook\n",
    "\n",
    "The notebook requires `nested-dask` and few other packages to be installed.\n",
    "- `lsdb` to load and join \"object\" (pointing) and \"source\" (detection) ZTF catalogs\n",
    "- `aiohttp` is `lsdb`'s optional dependency to download the data via web\n",
    "- `light-curve` to extract features from light curves\n",
    "- `matplotlib` to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1710055600582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:15:57.356540Z",
     "start_time": "2024-05-30T21:15:55.782198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line to install nested-dask\n",
    "# %pip install nested-dask\n",
    "\n",
    "# Comment the following line to skip dependencies installation\n",
    "#%pip install --quiet lsdb aiohttp light-curve matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be889d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --quiet git+https://github.com/astronomy-commons/lsdb.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d03aa76aeeb1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:15:59.778253Z",
     "start_time": "2024-05-30T21:15:57.358002Z"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array\n",
    "import dask.distributed\n",
    "import light_curve as licu\n",
    "import matplotlib.pyplot as plt\n",
    "import nested_pandas as npd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask_expr import from_legacy_dataframe\n",
    "from lsdb import read_hats\n",
    "from matplotlib.colors import LogNorm\n",
    "from nested_dask import NestedFrame\n",
    "import dask, lsdb, nested_dask\n",
    "\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10510ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Version Info\n",
    "print(f\"LSDB verion: {lsdb.__version__}\")\n",
    "print(f\"Nested-Dask verion: {nested_dask.__version__}\")\n",
    "print(f\"Dask verion: {dask.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7741336",
   "metadata": {},
   "source": [
    "# Configuration for this Particular Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 16 \n",
    "memory_per_worker = 20 # in GB\n",
    "threads_per_worker = 2 # We want 10 GB per worker thread\n",
    "partition_chunk_size = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25bdc4",
   "metadata": {},
   "source": [
    "Some additional setup for using Dask on epyc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.set({\n",
    "    'temporary-directory' :'/epyc/ssd/users/wbeebe/tmp',\n",
    "    'distributed.comm.timeouts.connect': '1200s',\n",
    "    'distributed.comm.timeouts.tcp': '1200s',\n",
    "    #'dataframe_optimize':None,\n",
    "    #'delayed_optimize':None,\n",
    "    #'dataframe.shuffle.method': shuffle_method,\n",
    "})\n",
    "\n",
    "dask.config.set({\"dataframe.shuffle-compression\": 'Snappy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169686259687cb2",
   "metadata": {},
   "source": [
    "## Load ZTF DR14\n",
    "For the demonstration purposes we use a light version of the ZTF DR14 catalog distributed by LINCC Frameworks, a half-degree circle around RA=180, Dec=10.\n",
    "We load the data from HTTPS as two LSDB catalogs: objects (metadata catalog) and source (light curve catalog)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6800b14",
   "metadata": {},
   "source": [
    "## Setup cone search\n",
    "Here we use a cone search for the LSDB load to keep our data small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsdb.core.search import ConeSearch\n",
    "# A tiny cone search\n",
    "search_area = ConeSearch(ra=254, dec=35, radius_arcsec=4.0 * 3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403e00e2fd8d081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:16:02.906421Z",
     "start_time": "2024-05-30T21:15:59.779007Z"
    }
   },
   "outputs": [],
   "source": [
    "catalogs_dir = \"/data3/epyc/data3/hats/catalogs/ztf_dr14\"\n",
    "\n",
    "\n",
    "lsdb_object = read_hats(\n",
    "    f\"{catalogs_dir}/ztf_object\",\n",
    "    columns=[\"ra\", \"dec\", \"ps1_objid\"],\n",
    "    search_filter=search_area,\n",
    ")\n",
    "lsdb_source = read_hats(\n",
    "    f\"{catalogs_dir}/ztf_source\",\n",
    "    columns=[\"mjd\", \"ra\", \"dec\", \"mag\", \"magerr\", \"band\", \"ps1_objid\", \"catflags\"],\n",
    "    #search_filter=search_area,\n",
    ")\n",
    "lc_columns = [\"mjd\", \"mag\", \"magerr\", \"band\", \"catflags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed4201f2c59f542",
   "metadata": {},
   "source": [
    "We need to merge these two catalogs to get the light curve data.\n",
    "It is done with LSDB's `.join_nested()` method which would give us a new catalog with a nested frame of ZTF sources. For this tutorial we'll just use the underlying nested dataframe for the rest of the analysis rather than the LSDB catalog directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edaade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nesting Sources into Object\n",
    "nested_ddf = lsdb_object.join_nested(lsdb_source, left_on=\"ps1_objid\", right_on=\"ps1_objid\", nested_column_name=\"lc\")\n",
    "\n",
    "# TODO remove once have added LSDB wrappers for nested_dask (reduce, dropna, etc)\n",
    "nested_ddf = nested_ddf._ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c32f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# of partitions: {nested_ddf.npartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9c6ceaf6bc3d2",
   "metadata": {},
   "source": [
    "## Convert LSDB joined catalog to `nested_dask.NestedFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f97583a3c1ba4",
   "metadata": {},
   "source": [
    "First, we plan the computation to convert the joined Dask DataFrame to a NestedFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6820724bcd4781",
   "metadata": {},
   "source": [
    "Now we filter our dataframe by the `catflags` column (0 flags correspond to the perfect observational conditions) and the `band` column to be equal to `r`.\n",
    "After filtering the detections, we are going to count the number of detections per object and keep only those objects with more than 10 detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bd831fc1f6f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:16:02.963360Z",
     "start_time": "2024-05-30T21:16:02.952708Z"
    }
   },
   "outputs": [],
   "source": [
    "r_band = nested_ddf.query(\"lc.catflags == 0 and lc.band == 'r'\")\n",
    "nobs = r_band.reduce(np.size, \"lc.mjd\", meta={0: int}).rename(columns={0: \"nobs\"})\n",
    "r_band = r_band[nobs[\"nobs\"] > 10]\n",
    "r_band"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f2d9052f5843f",
   "metadata": {},
   "source": [
    "Later we are going to extract features, so we need to prepare light-curve data to be in the same float format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6ec9f5b0889e1",
   "metadata": {},
   "source": [
    "### Extract features from ZTF light curves\n",
    "\n",
    "Now we are going to extract some features:\n",
    "- Top periodogram peak\n",
    "- Mean magnitude\n",
    "- Von Neumann's eta statistics\n",
    "- Excess variance statistics\n",
    "- Number of observations\n",
    "\n",
    "We are going to use [`light-curve`](https://github.com/light-curve/light-curve-python) package for this purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b9dbe67192b2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:16:02.978308Z",
     "start_time": "2024-05-30T21:16:02.964225Z"
    }
   },
   "outputs": [],
   "source": [
    "extractor = licu.Extractor(\n",
    "    licu.Periodogram(\n",
    "        peaks=1,\n",
    "        max_freq_factor=1.0, # Currently 1.0 for fast runs, will raise for more interesting graphs later\n",
    "        fast=True,\n",
    "    ),  # Would give two features: peak period and signa-to-noise ratio of the peak\n",
    ")\n",
    "\n",
    "\n",
    "# light-curve requires all arrays to be the same dtype.\n",
    "# It also requires the time array to be ordered and to have no duplicates.\n",
    "def extract_features(mjd, mag, **kwargs):\n",
    "    # We offset date, so we still would have <1 second precision\n",
    "    t = np.asarray(mjd - 60000, dtype=np.float32)\n",
    "    _, sort_index = np.unique(t, return_index=True)\n",
    "    features = extractor(\n",
    "        t[sort_index],\n",
    "        mag[sort_index],\n",
    "        **kwargs,\n",
    "    )\n",
    "    # Return the features as a dictionary\n",
    "    return dict(zip(extractor.names, features))\n",
    "\n",
    "\n",
    "features = r_band.reduce(\n",
    "    extract_features,\n",
    "    \"lc.mjd\",\n",
    "    \"lc.mag\",\n",
    "    meta={name: np.float32 for name in extractor.names},\n",
    ")\n",
    "\n",
    "df_w_features = r_band.join(features)\n",
    "df_w_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2db5bc7d5f23c",
   "metadata": {},
   "source": [
    "Before we are going next and actually run the computation, let's create a Dask client which would allow us to run the computation in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0e5c9816cd9f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:16:03.687894Z",
     "start_time": "2024-05-30T21:16:02.979043Z"
    }
   },
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(\n",
    "    n_workers=n_workers, \n",
    "    threads_per_worker=threads_per_worker, \n",
    "    memory_limit=f\"{memory_per_worker}GB\",\n",
    "    processes=True, # Does not use the threaded scheduler\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90fc50",
   "metadata": {},
   "source": [
    "## Chunking attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c63e1",
   "metadata": {},
   "source": [
    "This solution results in intractable memory usage and uses up too much time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6fb4857b9dc97d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:16:30.171468Z",
     "start_time": "2024-05-30T21:16:03.688907Z"
    }
   },
   "outputs": [],
   "source": [
    "# This solution results in intractable memory usage and uses up too much time\n",
    "\n",
    "now = datetime.now()\n",
    "start_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "\n",
    "intermediate_res = []\n",
    "#df_w_features = df_w_features.drop(columns=[\"period_0\", \"period_s_to_n_0\"])\n",
    "total_partitions = df_w_features.npartitions\n",
    "chunk_size = partition_chunk_size\n",
    "print(f\"{total_partitions} partitions chunked into {total_partitions//chunk_size + (0 if total_partitions % chunk_size == 0 else 1)} chunks with {chunk_size} partitions each\")\n",
    "for start_partition in range(0, total_partitions, chunk_size):\n",
    "    end_partition = min(start_partition + chunk_size, total_partitions)\n",
    "    curr_chunk = df_w_features.partitions[start_partition: end_partition]\n",
    "    intermediate_res.append(curr_chunk.compute())\n",
    "    print(f\"intermediate_res {type(intermediate_res[-1])}\")\n",
    "res_frame = dd.concat(intermediate_res)\n",
    "print(f\"res_frame {type(res_frame)}\")\n",
    "\n",
    "lg_period_bins = np.linspace(-1, 2, 101)\n",
    "lg_snr_bins = np.linspace(0, 2, 101)\n",
    "\n",
    "# TODO can we convert more efficiently here?\n",
    "lg_period = dask.array.log10(dask.array.asarray(res_frame[\"period_0\"].values))\n",
    "lg_snr = dask.array.log10(dask.array.asarray(res_frame[\"period_s_to_n_0\"].values))\n",
    "\n",
    "hist2d = dask.array.histogram2d(\n",
    "    lg_period,\n",
    "    lg_snr,\n",
    "    bins=[lg_period_bins, lg_snr_bins],\n",
    ")\n",
    "# Run the computation\n",
    "hist2d = hist2d[0].compute()\n",
    "\n",
    "# Plot the 2D histogramm,  j\n",
    "plt.imshow(\n",
    "    hist2d.T,\n",
    "    extent=(lg_period_bins[0], lg_period_bins[-1], lg_snr_bins[0], lg_snr_bins[-1]),\n",
    "    origin=\"lower\",\n",
    "    norm=LogNorm(vmin=1, vmax=hist2d.max()),\n",
    ")\n",
    "plt.colorbar(label=\"Number of stars\")\n",
    "plt.xlabel(\"lg Period/day\")\n",
    "plt.ylabel(\"lg S/N\")\n",
    "\n",
    "later = datetime.now()\n",
    "current_time = later.strftime(\"%m_%d_%H_%M_%S\")\n",
    "plt.savefig(f\"ztf_periodogram_chunk_start_{start_time}_end_{current_time}.png\")\n",
    "print(f\"Time elapsed in seconds: {later - now}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e5774",
   "metadata": {},
   "source": [
    "This attempts isolating the computation on a per-partition basis. It's slower than the solution below but actually completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up bins for histogram\n",
    "lg_period_bins = np.linspace(-1, 2, 101)\n",
    "lg_snr_bins = np.linspace(0, 2, 101)\n",
    "\n",
    "now = datetime.now()\n",
    "start_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "\n",
    "# Create empty histogram array for accumulation\n",
    "final_hist2d = np.zeros((len(lg_period_bins) - 1, len(lg_snr_bins) - 1))\n",
    "\n",
    "# Divide the DataFrame into delayed partitions\n",
    "partitions = df_w_features.to_delayed()\n",
    "\n",
    "for partition in partitions:\n",
    "    # Convert partition to a Pandas DataFrame\n",
    "    partition_df = dask.compute(partition)[0]\n",
    "\n",
    "    # Calculate log values and filter out non-positive values (since log10 can't take 0 or negative)\n",
    "    lg_period = np.log10(partition_df[\"period_0\"].replace(0, np.nan).dropna())\n",
    "    lg_snr = np.log10(partition_df[\"period_s_to_n_0\"].replace(0, np.nan).dropna())\n",
    "\n",
    "    # Compute partial histogram\n",
    "    hist2d, _, _ = np.histogram2d(\n",
    "        lg_period,\n",
    "        lg_snr,\n",
    "        bins=[lg_period_bins, lg_snr_bins],\n",
    "    )\n",
    "\n",
    "    # Accumulate into final histogram\n",
    "    final_hist2d += hist2d\n",
    "\n",
    "# Plot the accumulated 2D histogram\n",
    "plt.imshow(\n",
    "    final_hist2d.T,\n",
    "    extent=(lg_period_bins[0], lg_period_bins[-1], lg_snr_bins[0], lg_snr_bins[-1]),\n",
    "    origin=\"lower\",\n",
    "    norm=LogNorm(vmin=1, vmax=final_hist2d.max()),\n",
    ")\n",
    "plt.colorbar(label=\"Number of stars\")\n",
    "plt.xlabel(\"lg Period/day\")\n",
    "plt.ylabel(\"lg S/N\")\n",
    "\n",
    "later = datetime.now()\n",
    "current_time = later.strftime(\"%m_%d_%H_%M_%S\")\n",
    "plt.savefig(f\"ztf_periodogram_start_{start_time}_end_{current_time}.png\")\n",
    "print(f\"Time elapsed in seconds: {later - now}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376f5a9",
   "metadata": {},
   "source": [
    "## Construct per-partition histograms with to_delayed\n",
    "\n",
    "This actually completes on small workflows in a timely fashion and seems to help Dask be more memory efficient,\n",
    "but still needs to be tested at a larger scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up bins for histogram\n",
    "lg_period_bins = np.linspace(-1, 2, 101)\n",
    "lg_snr_bins = np.linspace(0, 2, 101)\n",
    "\n",
    "now = datetime.now()\n",
    "start_time = now.strftime(\"%m_%d_%H_%M_%S\")\n",
    "\n",
    "# Create empty histogram array for accumulation\n",
    "lg_period_bin_count = len(lg_period_bins) - 1\n",
    "lg_snr_bin_count = len(lg_snr_bins) - 1\n",
    "\n",
    "# Function to compute histogram for each partition\n",
    "def compute_partition_histogram(partition_df):\n",
    "    # Dropna and replace zeroes to handle log10 computation\n",
    "    period_0 = partition_df[\"period_0\"].replace(0, np.nan).dropna()\n",
    "    snr_0 = partition_df[\"period_s_to_n_0\"].replace(0, np.nan).dropna()\n",
    "\n",
    "    # If the partition is empty after dropna, return an empty histogram\n",
    "    if period_0.empty or snr_0.empty:\n",
    "        return np.zeros((lg_period_bin_count, lg_snr_bin_count))\n",
    "\n",
    "    # Convert this partition to a dask array so it can we can lazily evaluate the histogram\n",
    "    lg_period = dask.array.log10(dask.array.asarray(period_0))\n",
    "    lg_snr = dask.array.log10(dask.array.asarray(snr_0))\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    lg_period = np.log10(period_0)\n",
    "    lg_snr = np.log10(snr_0)\n",
    "\n",
    "    # Compute the histogram for the partition\n",
    "    hist2d, _, _ = np.histogram2d(\n",
    "        lg_period,\n",
    "        lg_snr,\n",
    "        bins=[lg_period_bins, lg_snr_bins]\n",
    "    )\n",
    "    return hist2d\n",
    "\n",
    "# Apply histogram computation to each partition\n",
    "partitions = df_w_features.to_delayed()\n",
    "delayed_histograms = [dask.delayed(compute_partition_histogram)(partition) for partition in partitions]\n",
    "\n",
    "# Use dask to combine all partition-level histograms\n",
    "total_hist2d = dask.delayed(sum)(delayed_histograms)\n",
    "\n",
    "# Compute the final combined histogram\n",
    "final_hist2d = total_hist2d.compute()\n",
    "\n",
    "# Plot the accumulated 2D histogram\n",
    "plt.imshow(\n",
    "    final_hist2d.T,\n",
    "    extent=(lg_period_bins[0], lg_period_bins[-1], lg_snr_bins[0], lg_snr_bins[-1]),\n",
    "    origin=\"lower\",\n",
    "    norm=LogNorm(vmin=1, vmax=final_hist2d.max()),\n",
    ")\n",
    "plt.colorbar(label=\"Number of stars\")\n",
    "plt.xlabel(\"lg Period/day\")\n",
    "plt.ylabel(\"lg S/N\")\n",
    "\n",
    "later = datetime.now()\n",
    "current_time = later.strftime(\"%m_%d_%H_%M_%S\")\n",
    "plt.savefig(f\"ztf_periodogram_start_{start_time}_end_{current_time}.png\")\n",
    "print(f\"Time elapsed in seconds: {later - now}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cfeb23acf497cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:16:47.499116Z",
     "start_time": "2024-05-30T21:16:47.075603Z"
    }
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "period_chunking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
