{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aea1ed5-9be5-4a7e-b9c2-aa5a23dc54c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process ZTF AXS (DR14) with LSDB and Tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d485791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import monotonic\n",
    "\n",
    "import light_curve as licu\n",
    "import lsdb\n",
    "import numpy as np\n",
    "from dask.distributed import Client\n",
    "from tape import Ensemble, ColumnMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ba99e-6c8a-46b2-a82e-668851d07189",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use LSDB to load 'object' and 'source' catalogs\n",
    "\n",
    "We do not really read or process anything (but some metadata) until `.compute()` is called in the very end\n",
    "\n",
    "We load few columns only, but actual analysis bellow doesn't really use them all\n",
    "\n",
    "Paths are for PSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c8d823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.8 s, sys: 2.81 s, total: 31.6 s\n",
      "Wall time: 37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "objects = lsdb.read_hipscat(\n",
    "    '/ocean/projects/phy210048p/shared/hipscat/catalogs/ztf_axs/ztf_dr14',\n",
    "    columns=['ps1_objid', 'ra', 'dec', 'nobs_g', 'nobs_r', 'nobs_i',\n",
    "             'Norder', 'Dir', 'Npix']\n",
    ")\n",
    "sources = lsdb.read_hipscat(\n",
    "    '/ocean/projects/phy210048p/shared/hipscat/catalogs/ztf_axs/ztf_source',\n",
    "    columns=['ps1_objid', 'mjd', 'mag', 'magerr', 'catflags', 'band',\n",
    "             'Norder', 'Dir', 'Npix']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeaaecb-56fa-46f9-8221-e6d8bc052921",
   "metadata": {},
   "source": [
    "### Use LSDB to join objects and sources\n",
    "\n",
    "This would assign sources object's `_hipscat_index`, which we are going to use as a primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64382a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "joined_sources = objects.join(\n",
    "    sources,\n",
    "    left_on='ps1_objid',\n",
    "    right_on='ps1_objid',\n",
    "    output_catalog_name='ztf_axs_sources'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d9d5b-866b-4681-bee0-9eae1e25f4b8",
   "metadata": {},
   "source": [
    "### Let's take only first few partitions of the object table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa1b94-1b5f-48e8-aaf6-b7520037ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_object_partitions = 5\n",
    "# off by one because we need last partition **end**\n",
    "last_object_division = objects._ddf.divisions[n_object_partitions+1]\n",
    "\n",
    "n_joined_source_partitions = np.searchsorted(joined_sources._ddf.divisions, last_object_division) - 1\n",
    "\n",
    "print(f'{n_object_partitions = } / {objects._ddf.npartitions}')\n",
    "print(f'{n_joined_source_partitions = } / {joined_sources._ddf.npartitions}')\n",
    "\n",
    "object_frame = objects._ddf.partitions[:n_object_partitions]\n",
    "source_frame = joined_sources._ddf.partitions[:n_joined_source_partitions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0537b8-7073-4399-8667-9660136115c7",
   "metadata": {},
   "source": [
    "### Create Dask client\n",
    "\n",
    "On a SLURM cluster we may use `dask_jobqueue.SLURMCluster` to scale our job.\n",
    "In this case the current node would be a manager node, and would run none of Dask workers itself.\n",
    "Instead it would run SLURM jobs, each with few workers, and assign Dask tasks for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf58b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a SLURM cluster\n",
    "\n",
    "# from dask_jobqueue import SLURMCluster\n",
    "\n",
    "# cluster = SLURMCluster(\n",
    "#     # Number of Dask workers per node\n",
    "#     processes=16,\n",
    "#     # Regular memory node type on PSC bridges2\n",
    "#     queue=\"RM\",\n",
    "#     # dask_jobqueue requires cores and memory to be specified\n",
    "#     # We set them to match RM specs\n",
    "#     cores=128,\n",
    "#     memory=\"256GB\",\n",
    "#     walltime=\"12:00:00\",\n",
    "# )\n",
    "# cluster.adapt(maximum_jobs=10)\n",
    "# client = Client(cluster)\n",
    "\n",
    "### Or create a local cluster\n",
    "client = Client()\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab02951-6acc-4d6e-8c3a-ab8c82be79a3",
   "metadata": {},
   "source": [
    "### Show how to access Dask dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1aa2f1-c997-4d5c-b8b2-cfd150da04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a command for dashboard ssh-tunneling\n",
    "\n",
    "import socket\n",
    "from getpass import getuser\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "local_addr = '127.0.0.1:8787'\n",
    "remote_host = 'bridges2.psc.edu'\n",
    "\n",
    "with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:\n",
    "    s.connect(('1.1.1.1', 53))\n",
    "    ip = s.getsockname()[0]\n",
    "username = getuser()\n",
    "dashboard_port = urlparse(client.dashboard_link).port\n",
    "\n",
    "print(f'''\n",
    "Copy-paste and run in your terminal:\n",
    "\n",
    "ssh -N -L {local_addr}:{ip}:{dashboard_port} {username}@{remote_host}\n",
    "\n",
    "And open this URL in your browser to see the dashboard:\n",
    "http://{local_addr}/\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aca60e-eb45-4e66-be33-bdb8edee8d33",
   "metadata": {},
   "source": [
    "### Create Tape Ensemble and plan the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd5b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ens = Ensemble(client)\n",
    "column_mapper = ColumnMapper(\n",
    "    id_col='_hipscat_index',\n",
    "    time_col='mjd_ztf_source',\n",
    "    flux_col='mag_ztf_source',\n",
    "    err_col='magerr_ztf_source',\n",
    "    band_col='band_ztf_source',\n",
    ")\n",
    "ens.from_dask_dataframe(\n",
    "    object_frame=object_frame,\n",
    "    source_frame=source_frame,\n",
    "    sorted=True,\n",
    "    sort=False,\n",
    "    sync_tables=True,\n",
    "    column_mapper=column_mapper,\n",
    ")\n",
    "\n",
    "ens.source.query('catflags_ztf_source == 0 & magerr_ztf_source > 0').update_ensemble()\n",
    "ens = ens.calc_nobs(by_band=False, label=\"nobs\", temporary=False)\n",
    "ens.object.query('nobs_total >= 200').update_ensemble()\n",
    "features = ens.batch(licu.Amplitude(), band_to_calc=None, label='features', compute=False)\n",
    "max_amplitude = features['amplitude'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893428b-b085-4c72-bf2b-dec8b05ead99",
   "metadata": {},
   "source": [
    "### Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "t = monotonic()\n",
    "result = max_amplitude.compute()\n",
    "dt = monotonic() - t\n",
    "dt, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02b0c8-f63e-46a6-8a47-95cc7c89a921",
   "metadata": {},
   "source": [
    "### Save results to disk, just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cb6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cluster_result.txt', 'w') as f:\n",
    "    f.write(f'{dt = }\\n{result = }\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc931fb-8e1a-4180-9de1-05acd08f877b",
   "metadata": {},
   "source": [
    "### Shut down Dask cluster\n",
    "\n",
    "If we run SLURM cluster it would also cancel all associated slurm jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea553c42-bef9-484f-9d68-e1e3a38cbf79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tape-venv-python3.9",
   "language": "python",
   "name": "tape-venv-python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
