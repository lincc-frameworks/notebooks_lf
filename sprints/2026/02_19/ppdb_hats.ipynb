{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6b1cb4",
   "metadata": {},
   "source": [
    "# PPDB HATS\n",
    "\n",
    "### Demo and data validation\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "1. Simulate two PPDB daily increments.\n",
    "3. Perform a full collection reimport.\n",
    "4. Validate the data: the deduplication of objects and aggregation of sources.\n",
    "\n",
    "Ran on USDF, 2/19/2026."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04832e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import lsdb\n",
    "import nested_pandas as npd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import date\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from ppdb_hats.config import get_default_config\n",
    "\n",
    "# Input directory for PPDB parquet files\n",
    "PPDB_LSST_DIR = Path(\"/sdf/scratch/rubin/ppdb/data/ppdb_lsstcam\")\n",
    "\n",
    "# Output directory for PPDB HATS catalogs\n",
    "PPDB_HATS_DIR = Path(\"/sdf/data/rubin/shared/lsdb_commissioning/ppdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf38553",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d719c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files_for_date(date: date):\n",
    "    \"\"\"Find input parquet for DIA object, source and forced source\"\"\"\n",
    "    date = date.strftime(\"%Y/%m/%d\")\n",
    "    obj_files = glob(f\"{PPDB_LSST_DIR}/{date}/**/DiaObject*.parquet\")\n",
    "    print(f\"Number of DiaObject files: {len(obj_files)}\")\n",
    "    src_files = glob(f\"{PPDB_LSST_DIR}/{date}/**/DiaSource*.parquet\")\n",
    "    print(f\"Number of DiaSource files: {len(src_files)}\")\n",
    "    forced_src_files = glob(f\"{PPDB_LSST_DIR}/{date}/**/DiaForcedSource*.parquet\")\n",
    "    print(f\"Number of DiaForcedSource files: {len(forced_src_files)}\")\n",
    "    return obj_files, src_files, forced_src_files\n",
    "\n",
    "def get_pipeline_config(**kwargs):\n",
    "    \"\"\"Build custom pipeline config\"\"\"\n",
    "    return get_default_config(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04fb84f",
   "metadata": {},
   "source": [
    "### Pre-existing catalog collection\n",
    "\n",
    "Let's assume we already have a base collection, imported with Npix directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1bec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_dir = PPDB_HATS_DIR / \"dia_object_collection\"\n",
    "collection = lsdb.open_catalog(collection_dir)\n",
    "assert collection.hc_structure.catalog_info.npix_suffix == \"/\"\n",
    "print(f\"Collection has {len(collection):,} points\")\n",
    "collection.plot_pixels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93544f9",
   "metadata": {},
   "source": [
    "It has data from Sept 6, 2025 to Jan 25, 2026. All the input parquet paths are stored under `input_paths`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394fa9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\"head\", \"-1\", f\"{collection_dir}/input_paths/dia_object.txt\"])\n",
    "subprocess.run([\"tail\", \"-1\", f\"{collection_dir}/input_paths/dia_object.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3093e813",
   "metadata": {},
   "source": [
    "### Daily increment\n",
    "\n",
    "We will increment with data from a couple of days (Jan 26 and Jan 28), one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729760b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppdb_hats import DailyPipeline\n",
    "config = get_pipeline_config(until_date=date(2026, 1, 26))\n",
    "DailyPipeline(config=config).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d7c65",
   "metadata": {},
   "source": [
    "#### Validation of first increment\n",
    "\n",
    "Let's validate that the data looks consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_files, src_files, forced_src_files = find_files_for_date(date(2026,1,26))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cd001",
   "metadata": {},
   "source": [
    "The `input_paths` have been updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76245df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\"tail\", \"-6\", f\"{collection_dir}/input_paths/dia_object.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment = lsdb.open_catalog(collection_dir)\n",
    "print(f\"The new collection has {len(increment):,} objects.\")\n",
    "print(f\"The increment added {len(increment) - len(collection):,} new objects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76c73b",
   "metadata": {},
   "source": [
    "All the new objects exist in the new collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_objects = npd.read_parquet(obj_files)\n",
    "unique_ids = new_objects[\"diaObjectId\"].unique()\n",
    "increment_ids = increment[\"diaObjectId\"].compute()\n",
    "assert set(unique_ids).issubset(set(increment_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca91296e",
   "metadata": {},
   "source": [
    "Let's look at a specific object (selected to have both sources and forced sources):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_obj = increment.query(f\"diaObjectId == 169738311479853178\")\n",
    "my_obj.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf266ad",
   "metadata": {},
   "source": [
    "The first entry is new PPDB data, with information on `i-band`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the original input object parquet\n",
    "new_objects.query(f\"diaObjectId == 169738311479853178\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a398633",
   "metadata": {},
   "source": [
    "Making sure this object's new sources were appended correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_obj[\"diaSource\"].compute().explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3481ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the original input source parquet\n",
    "npd.read_parquet(src_files).query(f\"diaObjectId == 169738311479853178\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e05dd",
   "metadata": {},
   "source": [
    "We keep track of the already imported parquet paths. On an increment, those are ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c131830a",
   "metadata": {},
   "source": [
    "#### Validation of second increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee109420",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_files, src_files, forced_src_files = find_files_for_date(date(2026, 1, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1709c",
   "metadata": {},
   "source": [
    "The pipeline will succeed when there's no new data for forced sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_pipeline_config(until_date=date(2026, 1, 28))\n",
    "DailyPipeline(config=config).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e51cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment2 = lsdb.open_catalog(collection_dir)\n",
    "print(f\"The collection has {len(increment2):,} objects.\")\n",
    "print(f\"The increment added {len(increment2) - len(increment):,} new objects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7312f39",
   "metadata": {},
   "source": [
    "We again did not lose data in this increment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c63ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_objects = npd.read_parquet(obj_files)\n",
    "unique_ids = new_objects[\"diaObjectId\"].unique()\n",
    "increment2_ids = increment2[\"diaObjectId\"].compute()\n",
    "assert set(unique_ids).issubset(set(increment2_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84993da7",
   "metadata": {},
   "source": [
    "Let's check an object which was seen in increment one and two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52580d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_obj = increment2.query(f\"diaObjectId == 169747015140900902\").compute()\n",
    "my_obj.sort_values(\"validityStartMjdTai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa7f0e",
   "metadata": {},
   "source": [
    "We appended the most recent object data from the input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047aa094",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_objects.query(f\"diaObjectId == 169747015140900902\").sort_values(\"validityStartMjdTai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6158a0e",
   "metadata": {},
   "source": [
    "After each increment we have a parquet file for each date, which we can query (even with other parquet readers) to get specific date updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2350fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\"ls\", f\"{collection_dir}/dia_object_lc/dataset/Norder=2/Dir=0/Npix=106\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68eef3",
   "metadata": {},
   "source": [
    "### Weekly reimport\n",
    "\n",
    "This reprocessing step creates a weekly \"release\" of the collection, with new margins and indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41461cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppdb_hats import WeeklyPipeline\n",
    "WeeklyPipeline().execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114308a",
   "metadata": {},
   "source": [
    "We have slightly less rows because we de-duplicated object data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c777c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_id = date.today().strftime(\"%Y%m%d\")\n",
    "weekly_dir = PPDB_HATS_DIR / \"weekly\" / f\"dia_object_collection_{weekly_id}\"\n",
    "weekly = lsdb.open_catalog(weekly_dir)\n",
    "print(f\"Weekly has ~{len(weekly) / len(increment2) * 100:.2f}% of incremented rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a05d8f",
   "metadata": {},
   "source": [
    "The IDs match for both collections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_ids = weekly[\"diaObjectId\"].compute()\n",
    "increment2_ids = increment2[\"diaObjectId\"].compute()\n",
    "unique_increment2_ids = np.unique(increment2_ids)\n",
    "assert len(weekly_ids) == len(unique_increment2_ids)\n",
    "assert set(weekly_ids) == set(unique_increment2_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e804f",
   "metadata": {},
   "source": [
    "Let's grab our first object and make sure that its sources were correctly merged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd46644",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_obj = weekly.id_search({\"diaObjectId\": 169738311479853178}).compute()\n",
    "weekly_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking sources\n",
    "increm_obj_source = increment2.query(\"diaObjectId == 169738311479853178\")[\"diaSource\"]\n",
    "expected_obj_source = increm_obj_source.explode().sort_values(\"midpointMjdTai\").compute()\n",
    "pd.testing.assert_frame_equal(expected_obj_source, weekly_obj[\"diaSource\"].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35536d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking forced sources\n",
    "increm_obj_source = increment2.query(\"diaObjectId == 169738311479853178\")[\"diaForcedSource\"]\n",
    "expected_obj_source = increm_obj_source.explode().sort_values(\"midpointMjdTai\").compute()\n",
    "pd.testing.assert_frame_equal(expected_obj_source, weekly_obj[\"diaForcedSource\"].explode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
