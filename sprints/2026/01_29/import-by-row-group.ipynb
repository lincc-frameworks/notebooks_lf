{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24112253",
   "metadata": {},
   "source": [
    "# Iterate by row group on import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ec96dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cloudpickle\n",
    "import pyarrow.dataset as pds\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from hats import read_hats\n",
    "from hats.pixel_math.sparse_histogram import SparseHistogram\n",
    "from hats_import.catalog.arguments import ImportArguments\n",
    "from hats_import.catalog.file_readers import ParquetPandasReader, ParquetPyarrowReader\n",
    "from hats_import.pipeline import pipeline_with_client\n",
    "\n",
    "\n",
    "hats_import_data_dir = Path(\"/Users/orl/code/lsdb-plus/hats-import/tests/data\")\n",
    "current_tmp_dir = Path(\"/Users/orl/code/lsdb-plus/liv-lf/09 - LSDB Sprint Demos/tmp\")\n",
    "\n",
    "if not current_tmp_dir.exists():\n",
    "    current_tmp_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f766addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test utilities\n",
    "\n",
    "\n",
    "def pickle_file_reader(tmp_path, file_reader) -> str:\n",
    "    \"\"\"Utility method to pickle a file reader, and return path to pickle.\"\"\"\n",
    "    pickled_reader_file = tmp_path / \"reader.pickle\"\n",
    "    with open(pickled_reader_file, \"wb\") as pickle_file:\n",
    "        cloudpickle.dump(file_reader, pickle_file)\n",
    "    return pickled_reader_file\n",
    "\n",
    "\n",
    "def read_partial_histogram(tmp_path, mapping_key, which_histogram=\"row_count\"):\n",
    "    \"\"\"Helper to read in the former result of a map operation.\"\"\"\n",
    "    histogram_file = tmp_path / f\"{which_histogram}_histograms\" / f\"{mapping_key}.npz\"\n",
    "    hist = SparseHistogram.from_file(histogram_file)\n",
    "    return hist.to_array()\n",
    "\n",
    "\n",
    "def dask_client():\n",
    "    \"\"\"Create a single client for use by all unit test cases.\"\"\"\n",
    "    cluster = LocalCluster(n_workers=1, threads_per_worker=1, dashboard_address=\":0\")\n",
    "    client = Client(cluster)\n",
    "    yield client\n",
    "    client.close()\n",
    "    cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0047de6",
   "metadata": {},
   "source": [
    "## Make multi row group data set\n",
    "\n",
    "Take the small sky object catalog and make a version with multiple row groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d88df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_row_groups = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091cfb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input data that will be used to generate the multi-row-group dataset.\n",
    "input_dataset_path = Path(hats_import_data_dir) / \"small_sky_object_catalog\" / \"dataset\"\n",
    "input_ds = pds.parquet_dataset(input_dataset_path / \"_metadata\")\n",
    "\n",
    "# Unit tests expect the Npix=11 data file\n",
    "input_frag = next(\n",
    "    frag for frag in input_ds.get_fragments() if frag.path.endswith(\"Npix=11.parquet\")\n",
    ")\n",
    "frag_key = Path(input_frag.path).relative_to(input_dataset_path)\n",
    "input_tbl = input_frag.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a2ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the multi-row-group parquet file.\n",
    "\n",
    "output_dataset_dir = Path(hats_import_data_dir) / \"test_formats\"\n",
    "output_dataset_path = current_tmp_dir / \"multi_row_group.parquet\"\n",
    "parquet_writer = pq.ParquetWriter(\n",
    "    where=output_dataset_path,\n",
    "    schema=input_tbl.schema,\n",
    "    use_dictionary=True,\n",
    "    compression=\"SNAPPY\",\n",
    ")\n",
    "step_size = (len(input_tbl) + num_row_groups - 1) // num_row_groups\n",
    "for i in range(0, len(input_tbl), step_size):\n",
    "    end = min(i + step_size, len(input_tbl))\n",
    "    batch = input_tbl.slice(i, end - i)\n",
    "    parquet_writer.write_table(batch)\n",
    "parquet_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f87a493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "_healpix_29: int64\n",
       "id: int64\n",
       "ra: double\n",
       "dec: double\n",
       "ra_error: int64\n",
       "dec_error: int64\n",
       "----\n",
       "_healpix_29: [[3187422220182231470,3187796123455121090,3188300701662669945,3188300701662669945,3192670279995812269,3192995164288065358,3194102393993053262,3195678697494500888,3196676706683767043,3196723640945762243,3197084959829715592],[3199487976390127826,3200256676290451752,3204516948860795443,3205876081882660000,3210595332878490279,3210618432891708849,3213763510984835044,3214195389015536144,3214969534743101722,3216746212972589575,3220523316512691843],...,[3372246530833947442,3380119216995778932,3380458994856416389,3388235695417907461,3388424365484869373,3389280889354717694,3389344265064945161,3389454143235220745,3390042224874323348,3390233494165235443,3390395511633005928],[3390927915493503682,3391172539243045430,3391463069396352355,3397177333029719609,3397704562975227384,3397804200316730633,3399000453069933430,3399532867186255393,3400255793565258227,3424180623569024089]]\n",
       "id: [[707,792,723,811,826,750,771,734,738,772,776],[733,804,747,739,816,703,794,735,797,815,748],...,[752,746,770,756,798,778,829,819,814,721,737],[799,825,796,754,806,791,824,702,767,743]]\n",
       "ra: [[308.5,320.5,315.5,315.5,335.5,338.5,348.5,348.5,345.5,348.5,344.5],[329.5,322.5,327.5,332.5,288.5,286.5,300.5,299.5,308.5,283.5,296.5],...,[291.5,283.5,285.5,319.5,316.5,313.5,314.5,313.5,312.5,314.5,316.5],[313.5,315.5,320.5,313.5,312.5,312.5,305.5,310.5,314.5,307.5]]\n",
       "dec: [[-69.5,-69.5,-68.5,-68.5,-69.5,-67.5,-67.5,-66.5,-64.5,-64.5,-63.5],[-65.5,-66.5,-61.5,-57.5,-69.5,-69.5,-66.5,-65.5,-62.5,-68.5,-63.5],...,[-34.5,-31.5,-29.5,-35.5,-36.5,-36.5,-35.5,-35.5,-33.5,-34.5,-33.5],[-31.5,-30.5,-33.5,-30.5,-29.5,-28.5,-28.5,-27.5,-29.5,-25.5]]\n",
       "ra_error: [[0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0],...,[0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0]]\n",
       "dec_error: [[0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0],...,[0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick check that the file was written...\n",
    "\n",
    "dataset_in = pq.ParquetDataset(output_dataset_path)\n",
    "dataset_in.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "271ab60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and that it was written with multiple row groups.\n",
    "\n",
    "file_in = pq.ParquetFile(output_dataset_path)\n",
    "file_in.num_row_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21089f84",
   "metadata": {},
   "source": [
    "## Read by row group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25663199",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_row_group_parquet = output_dataset_path\n",
    "\n",
    "num_row_groups = 12\n",
    "\n",
    "# Check number of row groups in test file.\n",
    "assert pq.ParquetFile(multi_row_group_parquet).num_row_groups == num_row_groups\n",
    "\n",
    "# Check we can iterate by row group, with ParquetPandasReader.\n",
    "total_row_groups = 0\n",
    "for row_group in ParquetPandasReader(iterate_by_row_groups=True).read(\n",
    "    multi_row_group_parquet\n",
    "):\n",
    "    total_row_groups += 1\n",
    "    assert len(row_group) > 0\n",
    "assert total_row_groups == num_row_groups\n",
    "\n",
    "# Check we can iterate by row group, with ParquetPyarrowReader.\n",
    "total_row_groups = 0\n",
    "for row_group in ParquetPyarrowReader(iterate_by_row_groups=True).read(\n",
    "    multi_row_group_parquet\n",
    "):\n",
    "    total_row_groups += 1\n",
    "    assert len(row_group) > 0\n",
    "assert total_row_groups == num_row_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a14b5",
   "metadata": {},
   "source": [
    "## Import while iterating by row group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b7cb49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_artifact_name = \"small_sky_source_catalog_by_row\"\n",
    "output_full_path = (current_tmp_dir / \"imported_catalog_by_row\").as_posix()\n",
    "if Path(output_full_path).exists():\n",
    "    import shutil\n",
    "\n",
    "    shutil.rmtree(output_full_path)\n",
    "\n",
    "args = ImportArguments(\n",
    "    output_artifact_name=\"small_sky_source_catalog_by_row\",\n",
    "    input_file_list=[multi_row_group_parquet],\n",
    "    file_reader=ParquetPyarrowReader(iterate_by_row_groups=True),\n",
    "    output_path=(current_tmp_dir / \"imported_catalog_by_row\").as_posix(),\n",
    "    highest_healpix_order=0,\n",
    "    progress_bar=False,\n",
    "    add_healpix_29=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "652f774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Client(n_workers=1, memory_limit=\"auto\") as client:\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55721333",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = read_hats(args.catalog_path)\n",
    "\n",
    "assert len(catalog) == len(input_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f4a0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
