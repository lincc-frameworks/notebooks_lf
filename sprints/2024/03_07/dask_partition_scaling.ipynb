{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsdb\n",
    "import ast\n",
    "from tape import Ensemble, ColumnMapper\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections.abc import Iterable\n",
    "\n",
    "dask.config.set({'temporary_directory': '/data/epyc/users/brantd/tmp'})\n",
    "dask.config.set({'dataframe.query-planning': False})\n",
    "\n",
    "from dask.distributed import Client, performance_report\n",
    "client = Client(n_workers=10, threads_per_worker=1,\n",
    "                memory_limit=\"60G\",\n",
    "                dashboard_address=':38764')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Investigation: Effect of Partition Sizes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a generator function\n",
    "def generate_data(num_points):\n",
    "    num_points = num_points\n",
    "    num_ids = num_points//5\n",
    "    all_bands = np.array([\"r\", \"g\", \"b\", \"i\"])\n",
    "    rows = {\n",
    "        \"id\": 8000 + (np.arange(num_points) % num_ids),\n",
    "        \"time\": np.arange(num_points),\n",
    "        \"flux\": np.arange(num_points) % len(all_bands),\n",
    "        \"band\": np.repeat(all_bands, num_points / len(all_bands)),\n",
    "        \"err\": 0.1 * (np.arange(num_points) % 10),\n",
    "        \"count\": np.arange(num_points),\n",
    "        \"something_else\": np.full(num_points, None),\n",
    "        }\n",
    "\n",
    "    ddf = dd.from_dict(rows, npartitions=1).set_index(\"id\", sort=True)\n",
    "\n",
    "    return ddf\n",
    "\n",
    "# Do a bunch of repartitions and write to parquet\n",
    "ddf = generate_data(100000).persist()\n",
    "partition_sizes = (\"100KB\", \"1MB\", \"10MB\") #, \"50MB\", \"100MB\", \"200MB\", \"500MB\")\n",
    "for size in partition_sizes:\n",
    "    size_ddf = ddf.repartition(partition_size=size)\n",
    "    size_ddf.to_parquet(f\"/data/epyc/users/brantd/data/dask_ps/{size}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Rough Task Graph Memory Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds size of objects\"\"\"\n",
    "\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "\n",
    "    return size\n",
    "\n",
    "def approx_memusage(df):\n",
    "    \"\"\"Approximates Task Graph Memory Usage\"\"\"\n",
    "\n",
    "    graph = df.__dask_graph__().to_dict()  # Convert to a dictionary\n",
    "    size = get_size(graph)\n",
    "    return size/1048576"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Basic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"/data/epyc/users/brantd/data/dask_ps/100KB/\"\n",
    "\n",
    "with performance_report(filename=\"/data/epyc/users/brantd/data/dask_ps/100KB_report.html\"):\n",
    "    dd.read_parquet(data_path).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b01adf3a9381a54eb60ad332d02f2bdb85b43467c9320a4a26e4c3681485a41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
