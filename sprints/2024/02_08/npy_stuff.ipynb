{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Speed up reading partition info\n",
       "\n",
       "Let's start with an existing catalog with a few thousand partitions in it. I've got a catwise metadata file locally, so I'm using it."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
       "from hipscat.catalog import PartitionInfo\n",
       "import os\n",
       "\n",
       "catalog_path = \"/data3/epyc/data3/hipscat/catalogs/neowise_yr8\"\n",
       "metadata_path = os.path.join(catalog_path, \"_metadata\")\n",
       "csv_path = os.path.join(catalog_path, \"partition_info.csv\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Start with reading from `_metadata`. This is pretty slow. We'll run each variety 20 times just to smooth out variance a little."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "CPU times: user 7.16 s, sys: 1.75 s, total: 8.91 s\n",
         "Wall time: 8.92 s\n"
        ]
       },
       {
        "data": {
         "text/plain": [
          "20010"
         ]
        },
        "execution_count": 2,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "%%time\n",
       "\n",
       "info = PartitionInfo.read_from_file(metadata_path)\n",
       "len(info.get_healpix_pixels())"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "CPU times: user 2min 11s, sys: 12.5 s, total: 2min 23s\n",
         "Wall time: 2min 23s\n"
        ]
       }
      ],
      "source": [
       "%%time\n",
       "\n",
       "for i in range(0,20):\n",
       "    info = PartitionInfo.read_from_file(metadata_path)\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
       "import numpy as np\n",
       "from hipscat.io import FilePointer, file_io\n",
       "from hipscat.pixel_math import HealpixPixel\n",
       "\n",
       "def write_to_npy(partition_info, catalog_path: FilePointer):\n",
       "    npy_file = file_io.append_paths_to_pointer(catalog_path, \"partition_info\")\n",
       "    data_frame = partition_info.as_dataframe()\n",
       "\n",
       "    npy_array = np.array(\n",
       "        [data_frame[partition_info.METADATA_ORDER_COLUMN_NAME], data_frame[partition_info.METADATA_PIXEL_COLUMN_NAME]]\n",
       "    )\n",
       "    np.save(npy_file, npy_array)\n",
       "\n",
       "def read_from_npy(catalog_path: FilePointer, storage_options: dict = None) -> PartitionInfo:\n",
       "    npy_file = file_io.append_paths_to_pointer(catalog_path, \"partition_info.npy\")\n",
       "    if not file_io.does_file_or_directory_exist(npy_file, storage_options=storage_options):\n",
       "        raise FileNotFoundError(f\"No partition info found where expected: {str(npy_file)}\")\n",
       "\n",
       "    npy_array = np.load(npy_file, allow_pickle=True)\n",
       "    orders = npy_array[0]\n",
       "    pixels = npy_array[1]\n",
       "\n",
       "    pixel_list = [HealpixPixel(order, pixel) for order, pixel in zip(orders, pixels)]\n",
       "\n",
       "    return PartitionInfo(pixel_list)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Using that same partition info, let's write it out to an uncompressed npy file. This should be wicked fast."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "CPU times: user 26.9 ms, sys: 1.78 ms, total: 28.6 ms\n",
         "Wall time: 27.9 ms\n"
        ]
       }
      ],
      "source": [
       "%%time\n",
       "write_to_npy(info, catalog_path)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "CPU times: user 20.6 ms, sys: 2.75 ms, total: 23.4 ms\n",
         "Wall time: 22 ms\n"
        ]
       },
       {
        "data": {
         "text/plain": [
          "20010"
         ]
        },
        "execution_count": 6,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "%%time\n",
       "\n",
       "info = read_from_npy(catalog_path)\n",
       "len(info.get_healpix_pixels())"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "CPU times: user 830 ms, sys: 6.64 ms, total: 836 ms\n",
         "Wall time: 835 ms\n"
        ]
       }
      ],
      "source": [
       "%%time\n",
       "\n",
       "for i in range(0,20):\n",
       "    info = read_from_npy(catalog_path)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Let's also look at what this looks like if we use a CSV and pandas parsing. Is it substantially worse than the npy?"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "CPU times: user 260 ms, sys: 6.21 ms, total: 266 ms\n",
         "Wall time: 289 ms\n"
        ]
       }
      ],
      "source": [
       "%%time\n",
       "info.write_to_file(csv_path)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "CPU times: user 51.6 ms, sys: 1.12 ms, total: 52.7 ms\n",
         "Wall time: 50.8 ms\n"
        ]
       },
       {
        "data": {
         "text/plain": [
          "20010"
         ]
        },
        "execution_count": 9,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "%%time\n",
       "\n",
       "info = PartitionInfo.read_from_csv(csv_path)\n",
       "len(info.get_healpix_pixels())"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "CPU times: user 960 ms, sys: 3.4 ms, total: 963 ms\n",
         "Wall time: 962 ms\n"
        ]
       }
      ],
      "source": [
       "%%time\n",
       "\n",
       "for i in range(0,20):\n",
       "    info = PartitionInfo.read_from_csv(csv_path)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Well. That's embarassing. It's really not much better than the original CSV stuff we had a few months ago. What if we pickle the whole list of pixels? And don't have to do any additional data marshalling?"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
       "def write_to_npy(partition_info, catalog_path: FilePointer):\n",
       "    npy_file = file_io.append_paths_to_pointer(catalog_path, \"partition_info\")\n",
       "    data_frame = partition_info.as_dataframe()\n",
       "\n",
       "    npy_array = np.array(\n",
       "        partition_info.pixel_list\n",
       "    )\n",
       "    np.save(npy_file, npy_array)\n",
       "\n",
       "def read_from_npy(catalog_path: FilePointer, storage_options: dict = None) -> PartitionInfo:\n",
       "    npy_file = file_io.append_paths_to_pointer(catalog_path, \"partition_info.npy\")\n",
       "    if not file_io.does_file_or_directory_exist(npy_file, storage_options=storage_options):\n",
       "        raise FileNotFoundError(f\"No partition info found where expected: {str(npy_file)}\")\n",
       "\n",
       "    npy_array = np.load(npy_file, allow_pickle=True)\n",
       "    return PartitionInfo(npy_array)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "CPU times: user 78.6 ms, sys: 323 Âµs, total: 78.9 ms\n",
         "Wall time: 77.1 ms\n"
        ]
       }
      ],
      "source": [
       "%%time\n",
       "write_to_npy(info, catalog_path)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "CPU times: user 30.4 ms, sys: 5.28 ms, total: 35.7 ms\n",
         "Wall time: 33 ms\n"
        ]
       },
       {
        "data": {
         "text/plain": [
          "20010"
         ]
        },
        "execution_count": 13,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "%%time\n",
       "\n",
       "info = read_from_npy(catalog_path)\n",
       "len(info.get_healpix_pixels())"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "CPU times: user 738 ms, sys: 7.11 ms, total: 745 ms\n",
         "Wall time: 740 ms\n"
        ]
       }
      ],
      "source": [
       "%%time\n",
       "\n",
       "for i in range(0,20):\n",
       "    info = read_from_npy(catalog_path)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "That's a little better, but we're not talking about a 10x speedup here. This is <2x speedup, that comes with a lot of complexity, and adding a new file type to the mix.\n",
       "\n",
       "So, I think it's just not worth it. And using the `partition_info.csv` that we already generate and know how to parse is pretty good."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Melissa LSDB",
      "language": "python",
      "name": "mmd11_lsdb"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }