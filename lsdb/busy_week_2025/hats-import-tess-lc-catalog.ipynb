{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44086aca-20db-44f5-8e05-0e6396731239",
   "metadata": {},
   "source": [
    "# Import TESS light curves into HATS catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98025ac1-80a7-4825-87c7-292d56f8c69e",
   "metadata": {},
   "source": [
    "## Install deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a242a9d3-a685-4cb0-af48-a7750e599682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U lsdb hats-import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d3856-d9fc-4f14-810e-9e8dd4628a84",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "For this notebook we need\n",
    "- `os`\n",
    "- `upath`\n",
    "- `collections`\n",
    "- `datetime`\n",
    "- `numpy`\n",
    "- `tqdm`\n",
    "- `pyarrow`\n",
    "- `astropy`\n",
    "- `hats_import`\n",
    "- `lsdb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a5d25-ca8d-4d67-a8e6-9edf50d3d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from upath import UPath\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from hats_import.catalog.file_readers import InputReader\n",
    "from hats_import import CollectionArguments\n",
    "import lsdb\n",
    "from dask.distributed import Client\n",
    "from hats_import.pipeline import pipeline_with_client, pipeline\n",
    "\n",
    "\n",
    "SH_ROOT = \"./sh_files\" # Path to cURL scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e5edd-90c5-4dbb-a58b-3ace73f3139c",
   "metadata": {},
   "source": [
    "## Download all TESS Light Curve cURL scripts, one per sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa457b5-5798-4ac4-a172-496cb91c1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell only needs to be run once, or if there are new sectors to download. \n",
    "# If the cURL scripts are already there, skip it!\n",
    "curl_root = UPath(\"https://archive.stsci.edu/missions/tess/download_scripts/sector\")\n",
    "LAST_SECTOR = 96\n",
    "\n",
    "def download_sh_files():\n",
    "    sh_root_path = UPath(SH_ROOT)\n",
    "    sh_root_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    for sector in tqdm(range(1, LAST_SECTOR+1)):\n",
    "        fname = f\"tesscurl_sector_{sector}_lc.sh\"\n",
    "        sh_remote = curl_root / fname\n",
    "        sh_local = sh_root_path / fname\n",
    "        with sh_local.open(\"wb\") as local:\n",
    "            local.write(sh_remote.read_bytes())\n",
    "\n",
    "\n",
    "download_sh_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff13029-62bc-4a4e-a342-6058f1ab30e1",
   "metadata": {},
   "source": [
    "## Create custom HATS-import reader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e1dbb-6950-4cee-a90e-84978b56dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_paths = sorted(upath.path for upath in UPath(SH_ROOT).glob(\"*.sh\"))\n",
    "s3_root = \"s3://stpubdata/tess/public/tid\"\n",
    "\n",
    "def slashfill(tic_str):\n",
    "    assert len(tic_str) == 16\n",
    "    result = \"/\".join(tic_str[i:i+4] for i in range(0, len(tic_str), 4))\n",
    "    return result\n",
    "\n",
    "def convert_uri(file_uri, return_id=True):\n",
    "    filename = os.path.basename(file_uri)\n",
    "    fname_list = filename.split(\"-\")\n",
    "    sector_str = fname_list[1]\n",
    "    tic_str = fname_list[2]\n",
    "\n",
    "    meta_path = sector_str + \"/\" + slashfill(tic_str)\n",
    "    s3_uri = UPath(\n",
    "        s3_root + \"/\" + meta_path + \"/\" + filename,\n",
    "        anon=True\n",
    "    )\n",
    "    if return_id:\n",
    "        return s3_uri, \"tic\" + \"_\".join([tic_str, sector_str])\n",
    "    return s3_uri\n",
    "\n",
    "def get_uris(lines, return_id):\n",
    "    \"\"\"\n",
    "    Given the lines from a TESS cURL script, convert the file URIs (which\n",
    "    currently point to STScI on-prem storage) to s3 cloud URIs. The cURL scripts\n",
    "    will soon be replaced with s3 URIs, so this step will become obsolete.\n",
    "    \"\"\"\n",
    "    uris = []\n",
    "    \n",
    "    for l in lines[1:]:\n",
    "        cmds = l.split()\n",
    "        uri = convert_uri(cmds[-1], return_id=return_id)\n",
    "        uris.append(uri)\n",
    "    \n",
    "    return uris\n",
    "\n",
    "\n",
    "class TESSLCReader(InputReader):\n",
    "    header0_columns = {\n",
    "        \"RA_OBJ\": pa.float64(),\n",
    "        \"DEC_OBJ\": pa.float64(),\n",
    "        \"TSTART\": pa.float32(),\n",
    "        \"TSTOP\": pa.float32(),\n",
    "        \"DATE-OBS\": pa.timestamp(\"ms\", tz=\"utc\"),\n",
    "        \"DATE-END\": pa.timestamp(\"ms\", tz=\"utc\"),\n",
    "        \"TICID\": pa.int64(),\n",
    "        \"SECTOR\": pa.int32(),\n",
    "        \"CAMERA\": pa.int8(),\n",
    "        \"CCD\": pa.int8(),\n",
    "        \"PMRA\": pa.float32(),\n",
    "        \"PMDEC\": pa.float32(),\n",
    "        \"PMTOTAL\": pa.float32(),\n",
    "        \"TEFF\": pa.float32(),\n",
    "        \"LOGG\": pa.float32(),\n",
    "        \"MH\": pa.float32(),\n",
    "        \"RADIUS\": pa.float32(),\n",
    "    }\n",
    "\n",
    "    header1_columns = {\n",
    "        \"EXPOSURE\": pa.float32(),\n",
    "        \"TELAPSE\": pa.float32(),\n",
    "        \"DEADC\": pa.float32(),\n",
    "        \"TIMEPIXR\": pa.float32(),\n",
    "        \"TIERRELA\": pa.float32(),\n",
    "        \"INT_TIME\": pa.float32(),\n",
    "        \"READTIME\": pa.float32(),\n",
    "        \"FRAMETIM\": pa.float32(),\n",
    "        \"NUM_FRM\": pa.int32(),\n",
    "        \"TIMEDEL\": pa.float32(),\n",
    "        \"GAINA\": pa.float32(),\n",
    "        \"GAINB\": pa.float32(),\n",
    "        \"GAINC\": pa.float32(),\n",
    "        \"GAIND\": pa.float32(),\n",
    "        \"READNOIA\": pa.float32(),\n",
    "        \"READNOIB\": pa.float32(),\n",
    "        \"READNOIC\": pa.float32(),\n",
    "        \"READNOID\": pa.float32(),\n",
    "        \"NREADOUT\": pa.int32(),\n",
    "        \"CDPP0_5\": pa.float32(),\n",
    "        \"CDPP1_0\": pa.float32(),\n",
    "        \"CDPP2_0\": pa.float32(),\n",
    "        \"CROWDSAP\": pa.float32(),\n",
    "        \"FLFRCSAP\": pa.float32(),\n",
    "        \"PDCVAR\": pa.float32(),\n",
    "        \"PR_GOOD1\": pa.float32(),\n",
    "        \"PR_WGHT1\": pa.float32(),\n",
    "        \"PR_GOOD2\": pa.float32(),\n",
    "        \"PR_WGHT2\": pa.float32(),\n",
    "        \"PR_GOOD3\": pa.float32(),\n",
    "        \"PR_WGHT3\": pa.float32(),\n",
    "        \"PDC_TOT\":  pa.float32(),\n",
    "        \"PDC_TOTP\": pa.float32(),\n",
    "        \"PDC_COR\": pa.float32(),\n",
    "        \"PDC_CORP\":pa.float32(),\n",
    "        \"PDC_VAR\":pa.float32(),\n",
    "        \"PDC_VARP\": pa.float32(),\n",
    "        \"PDC_NOI\": pa.float32(),\n",
    "        \"PDC_NOIP\": pa.float32(),\n",
    "        \"PDC_EPT\":pa.float32(),\n",
    "        \"PDC_EPTP\": pa.float32(),\n",
    "    }\n",
    "\n",
    "    header2_columns = {\n",
    "        \"NPIXSAP\": pa.int32(),\n",
    "        \"NPIXMISS\": pa.int32(),\n",
    "    }\n",
    "\n",
    "    def __init__(self, chunksize: int = 100, few_rows_per_sector: bool = False):\n",
    "        super().__init__()\n",
    "        self.chunksize = chunksize\n",
    "        self.few_rows_per_sector = few_rows_per_sector \n",
    "    \n",
    "    @staticmethod\n",
    "    def fits_to_hats_colname(name: str) -> str:\n",
    "        return name.lower().replace(\"-\", \"_\")\n",
    "    \n",
    "    def read(self, input_file: str, read_columns=None):\n",
    "        uris = self.get_uris_from_sh(input_file)\n",
    "\n",
    "        if self.few_rows_per_sector:\n",
    "            uris = uris[:500]\n",
    "\n",
    "        n_chunks = int(np.ceil(len(uris) / self.chunksize))\n",
    "        \n",
    "        for chunk in np.array_split(uris, n_chunks):\n",
    "            # Just ra and dec are needed\n",
    "            if read_columns is None:\n",
    "                yield self.get_whole_table(chunk)\n",
    "            else:\n",
    "                yield self.get_ra_dec_table(chunk, read_columns)\n",
    "\n",
    "    def get_ra_dec_table(self, uris, columns):\n",
    "        ra_, dec_ = [], []\n",
    "        for upath in uris:\n",
    "            ra, dec = self.get_radec_from_path(upath)\n",
    "            ra_.append(ra)\n",
    "            dec_.append(dec)\n",
    "        return pa.table(dict(zip(columns, [ra_, dec_], strict=True)))\n",
    "\n",
    "    def get_whole_table(self, uris):\n",
    "        data = defaultdict(list)\n",
    "        for path in uris:\n",
    "            try:\n",
    "                with path.open('rb') as fh, fits.open(fh) as hdul:\n",
    "                    # Adding values from headers\n",
    "                    self.add_header_values(data, hdul[0].header, self.header0_columns)\n",
    "                    self.add_header_values(data, hdul[1].header, self.header1_columns)\n",
    "                    self.add_header_values(data, hdul[2].header, self.header2_columns)\n",
    "    \n",
    "                    # Adding light curve\n",
    "                    data['lightcurve'].append(self.fits_table_to_pa_scalar(hdul[1].data))\n",
    "    \n",
    "                    # Adding aperture and its shape\n",
    "                    ap, ap_x, ap_y = self.parse_fist_aperture(hdul[2].data)\n",
    "                    data['aperture'].append(ap)\n",
    "                    data['aperture_size_x'].append(ap_x)\n",
    "                    data['aperture_size_y'].append(ap_y)\n",
    "            except (ValueError, RuntimeError):\n",
    "                print(\"Error reading\", path)\n",
    "                raise\n",
    "        table = pa.table(data)\n",
    "        return table\n",
    "\n",
    "    def add_header_values(self, data, header, columns):\n",
    "        for fits_colname, ty in columns.items():\n",
    "            hats_colname = self.fits_to_hats_colname(fits_colname)\n",
    "            value = header.get(fits_colname)\n",
    "            if pa.types.is_timestamp(ty):\n",
    "                try:\n",
    "                    value = datetime.fromisoformat(value)\n",
    "                except ValueError:\n",
    "                    # Two files have improperly formatted DATE-OBS values. \n",
    "                    # This catches and fixes the values on ingest.\n",
    "                    print(f\"ERROR: TIC {header['TICID']} sector {header['SECTOR']}, column {fits_colname}: {value}\")\n",
    "                    base = value.replace(\":60.000\", \":59.000\")\n",
    "                    value = datetime.fromisoformat(base) + timedelta(seconds=1)\n",
    "            elif pa.types.is_floating(ty):\n",
    "                if value is not None and np.isnan(value):\n",
    "                    value = None\n",
    "            data[hats_colname].append(pa.scalar(value, type=ty))\n",
    "        \n",
    "    @staticmethod\n",
    "    def fits_table_to_pa_scalar(arr):\n",
    "        data = {}\n",
    "        for fits_field, (fits_dtype, _) in arr.dtype.fields.items():\n",
    "            hats_field = TESSLCReader.fits_to_hats_colname(fits_field)\n",
    "            # Swap bytes to \"native\" order\n",
    "            hats_dtype = fits_dtype.newbyteorder('=')\n",
    "            data[hats_field] = np.asarray(arr[fits_field], dtype=hats_dtype)\n",
    "        return pa.scalar(data)\n",
    "        \n",
    "    def get_radec_from_path(self, path):\n",
    "        try:\n",
    "            with path.open('rb') as fh:\n",
    "                header = fits.getheader(fh, 0)\n",
    "        except (ValueError, RuntimeError):\n",
    "            print(\"Error getting coordinates for\", path)\n",
    "            raise\n",
    "        return header.get('RA_OBJ'), header.get('DEC_OBJ')\n",
    "    \n",
    "    def get_uris_from_sh(self, sh_file):\n",
    "        with open(sh_file) as fh:\n",
    "            return get_uris(fh.readlines(), return_id=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_fist_aperture(data):\n",
    "        dtype = data.dtype.newbyteorder(\"=\")\n",
    "        flat_array = np.asarray(data.flatten(), dtype=dtype)\n",
    "        return (\n",
    "            pa.scalar(flat_array),\n",
    "            pa.scalar(data.shape[1], type=pa.int16()),\n",
    "            pa.scalar(data.shape[0], type=pa.int16()),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a0c92-af1f-4239-8544-55df94b31f1f",
   "metadata": {},
   "source": [
    "## Define default columns and pipeline arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3397de21-ab30-45c7-9fb7-7d3ded4c4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_columns = [\n",
    "    'ra_obj',\n",
    "    'dec_obj',\n",
    "    'ticid',\n",
    "    'sector',\n",
    "    # CHANGE TO SUBCOLUMNS\n",
    "    'lightcurve.time',\n",
    "    'lightcurve.pdcsap_flux',\n",
    "    'lightcurve.pdcsap_flux_err',\n",
    "    'lightcurve.sap_flux',\n",
    "    'lightcurve.sap_flux_err',\n",
    "    'lightcurve.quality',\n",
    "]\n",
    "\n",
    "args = (\n",
    "    CollectionArguments(\n",
    "        output_artifact_name=\"tess_lightcurve_hats\",\n",
    "        output_path=\"hats\",\n",
    "    )\n",
    "    .catalog(\n",
    "        # CHANGE TO USE ALL SECTORS\n",
    "        input_file_list=sh_paths,\n",
    "        # REMOVE few_rows_per_sector=True\n",
    "        file_reader=TESSLCReader(chunksize=200, few_rows_per_sector=False),\n",
    "        ra_column=\"ra_obj\",\n",
    "        dec_column=\"dec_obj\",\n",
    "        sort_columns=\"ticid\",\n",
    "        highest_healpix_order=10,\n",
    "        pixel_threshold=500,\n",
    "        addl_hats_properties={\n",
    "            'hats_cols_default': default_columns,\n",
    "        },\n",
    "    )\n",
    "    # UNCOMMENT MARGINS\n",
    "     .add_margin(margin_threshold=10.0, is_default=True)\n",
    "    #.add_margin(margin_threshold=60.0)\n",
    "    .add_index(indexing_column=\"ticid\", include_healpix_29=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf474a1-5346-4cf6-8d44-f28c3e0ad448",
   "metadata": {},
   "source": [
    "## Run the import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8edfd-c8eb-4377-81ab-01e7a23a6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with Client(n_workers=os.cpu_count(), threads_per_worker=2, memory_limit=None) as client:\n",
    "    #display(client)\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52334693-de83-4192-90dc-5b050dbf690c",
   "metadata": {},
   "source": [
    "## Inspect the HATS catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b2831-fe67-4026-827f-566c4a67dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess = lsdb.open_catalog('hats/tess_lightcurve_hats')\n",
    "print(len(tess))\n",
    "tess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2319aa-da5d-44cc-b210-9533369546b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "tess.plot_pixels()\n",
    "plt.savefig(\"hats_full.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8127f-2d99-4378-b471-10f7994f7382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
