{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6171e5bbd47ce869",
   "metadata": {},
   "source": [
    "# Tansu Nested-Dask Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df8dbc",
   "metadata": {},
   "source": [
    "A basic demo that takes an [outlier detection method provided by Tansu Daylan](https://www.google.com/url?q=https://github.com/tdaylan/miletos/blob/71f7d18542f3ef82808eba588bad6361c8297351/miletos/main.py%23L5192&sa=D&source=docs&ust=1715889313765253&usg=AOvVaw2elaC5NDhYWMIVOZa-rq90) and applies it to a subset of a HiPSCatted ZTF catalog.\n",
    "\n",
    "# Load large catalog data with LSDB\n",
    "\n",
    "Here we load a small part of ZTF DR14 stored as HiPSCat catalog using [LSDB](https://lsdb.readthedocs.io/).\n",
    "\n",
    "The notebook is an adaptation of [the tutorial](https://github.com/lincc-frameworks/Rare_Gems_Demo/blob/main/Notebook_2_Basic_Time_Domain.ipynb) presented by Neven Caplar at the Rare Gems in Big Data conference, May 2024. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055a44b8ce3b34",
   "metadata": {},
   "source": [
    "## Install dependencies for the notebook\n",
    "\n",
    "The notebook requires `nested-dask` and few other packages to be installed.\n",
    "- `lsdb` to load and join \"object\" (pointing) and \"source\" (detection) ZTF catalogs\n",
    "- `aiohttp` is `lsdb`'s optional dependency to download the data via web\n",
    "- `light-curve` to extract features from light curves\n",
    "- `matplotlib` to plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79930f7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1710055600582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:15:57.356540Z",
     "start_time": "2024-05-30T21:15:55.782198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line to install nested-dask\n",
    "# %pip install nested-dask\n",
    "\n",
    "# Comment the following line to skip dependencies installation\n",
    "#%pip install --quiet lsdb matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d03aa76aeeb1c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:15:59.778253Z",
     "start_time": "2024-05-30T21:15:57.358002Z"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array\n",
    "import dask.distributed\n",
    "import matplotlib.pyplot as plt\n",
    "import nested_pandas as npd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask_expr import from_legacy_dataframe\n",
    "from lsdb import read_hipscat\n",
    "from matplotlib.colors import LogNorm\n",
    "from nested_dask import NestedFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169686259687cb2",
   "metadata": {},
   "source": [
    "## Load ZTF DR14\n",
    "For the demonstration purposes we use a light version of the ZTF DR14 catalog distributed by LINCC Frameworks, a half-degree circle around RA=180, Dec=10.\n",
    "We load the data from HTTPS as two LSDB catalogs: objects (metadata catalog) and source (light curve catalog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403e00e2fd8d081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:16:02.906421Z",
     "start_time": "2024-05-30T21:15:59.779007Z"
    }
   },
   "outputs": [],
   "source": [
    "catalogs_dir = \"https://epyc.astro.washington.edu/~lincc-frameworks/half_degree_surveys/ztf/\"\n",
    "\n",
    "lsdb_object = read_hipscat(\n",
    "    f\"{catalogs_dir}/ztf_object\",\n",
    "    columns=[\"ra\", \"dec\", \"ps1_objid\"],\n",
    ")\n",
    "lsdb_source = read_hipscat(\n",
    "    f\"{catalogs_dir}/ztf_source\",\n",
    "    columns=[\"mjd\", \"mag\", \"magerr\", \"band\", \"ps1_objid\", \"catflags\"],\n",
    ")\n",
    "lc_columns = [\"mjd\", \"mag\", \"magerr\", \"band\", \"catflags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed4201f2c59f542",
   "metadata": {},
   "source": [
    "We need to merge these two catalogs to get the light curve data.\n",
    "It is done with LSDB's `.join()` method which would give us a new catalog with all the columns from both catalogs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b57bced7f810c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:16:02.931031Z",
     "start_time": "2024-05-30T21:16:02.907834Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can ignore warning here - for this particular case we don't need margin cache\n",
    "lsdb_joined = lsdb_object.join(\n",
    "    lsdb_source,\n",
    "    left_on=\"ps1_objid\",\n",
    "    right_on=\"ps1_objid\",\n",
    "    suffixes=(\"\", \"\"),\n",
    ")\n",
    "joined_ddf = lsdb_joined._ddf\n",
    "joined_ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9c6ceaf6bc3d2",
   "metadata": {},
   "source": [
    "## Convert LSDB joined catalog to `nested_dask.NestedFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f97583a3c1ba4",
   "metadata": {},
   "source": [
    "First, we plan the computation to convert the joined Dask DataFrame to a NestedFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522ce0977ff9fdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:16:02.951964Z",
     "start_time": "2024-05-30T21:16:02.931819Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_nested_frame(df: pd.DataFrame, nested_columns: list[str]):\n",
    "    other_columns = [col for col in df.columns if col not in nested_columns]\n",
    "\n",
    "    # Since object rows are repeated, we just drop duplicates\n",
    "    object_df = df[other_columns].groupby(level=0).first()\n",
    "    nested_frame = npd.NestedFrame(object_df)\n",
    "\n",
    "    source_df = df[nested_columns]\n",
    "    # lc is for light curve\n",
    "    return nested_frame.add_nested(source_df, \"lc\")\n",
    "\n",
    "\n",
    "ddf = joined_ddf.map_partitions(\n",
    "    lambda df: convert_to_nested_frame(df, nested_columns=lc_columns),\n",
    "    meta=convert_to_nested_frame(joined_ddf._meta, nested_columns=lc_columns),\n",
    ")\n",
    "nested_ddf = NestedFrame.from_dask_dataframe(from_legacy_dataframe(ddf))\n",
    "nested_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_ddf.index.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6820724bcd4781",
   "metadata": {},
   "source": [
    "Now we filter our dataframe by the `catflags` column (0 flags correspond to the perfect observational conditions) and the `band` column to be equal to `r`.\n",
    "After filtering the detections, we are going to count the number of detections per object and keep only those objects with more than 10 detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bd831fc1f6f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T21:16:02.963360Z",
     "start_time": "2024-05-30T21:16:02.952708Z"
    }
   },
   "outputs": [],
   "source": [
    "r_band = nested_ddf.query(\"lc.catflags == 0 and lc.band == 'r'\")\n",
    "nobs = r_band.reduce(np.size, \"lc.mjd\", meta={0: int}).rename(columns={0: \"nobs\"})\n",
    "r_band = r_band[nobs[\"nobs\"] > 100]\n",
    "r_band"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f2d9052f5843f",
   "metadata": {},
   "source": [
    "Later we are going to extract features, so we need to prepare light-curve data to be in the same float format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6ec9f5b0889e1",
   "metadata": {},
   "source": [
    "### Extract outliers from ZTF light curves\n",
    "\n",
    "Here we have an example function courtesy of Tansu Daylan for outlier generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a39175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def srch_outlperi(time,  # time of samples\n",
    "                  flux, # relative flux of samples\n",
    "                  stdvflux, # relative flux error of samples\n",
    "                  n_outliers=5, # number of outliers to include in the search\n",
    "                  verbose=True, # Boolean flag to diagnose\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    Search for periodic outliers in a computationally efficient way\n",
    "    \"\"\"\n",
    "    \n",
    "    # indices of the outliers\n",
    "    indxtimesort = np.argsort(flux)[::-1][:n_outliers]\n",
    "    \n",
    "    # the times of the outliers\n",
    "    timeoutl = time[indxtimesort]\n",
    "    \n",
    "    # number of differences between times of outlier samples\n",
    "    numbdiff = int(n_outliers * (n_outliers - 1) / 2)\n",
    "    \n",
    "    # differences between times of outlier samples\n",
    "    difftimeoutl = np.empty(numbdiff)\n",
    "    \n",
    "    # compute the differences between times of outlier samples\n",
    "    listtemp = []\n",
    "    c = 0\n",
    "    indxoutl = np.arange(n_outliers)\n",
    "    for a in indxoutl:\n",
    "        for b in indxoutl:\n",
    "            if a >= b:\n",
    "                continue\n",
    "            listtemp.append([a, b])\n",
    "            difftimeoutl[c] = abs(timeoutl[a] - timeoutl[b])\n",
    "            c += 1\n",
    "    \n",
    "    # incides that sort the differences between times of outlier samples\n",
    "    indxsort = np.argsort(difftimeoutl)\n",
    "    \n",
    "    # sorted differences between times of outlier samples\n",
    "    difftimeoutlsort = difftimeoutl[indxsort]\n",
    "\n",
    "    # fractional differences between differences of times of outlier samples\n",
    "    frddtimeoutlsort = (difftimeoutlsort[1:] - difftimeoutlsort[:-1]) / ((difftimeoutlsort[1:] + difftimeoutlsort[:-1]) / 2.)\n",
    "\n",
    "    # index of the minimum fractional difference between differences of times of outlier samples\n",
    "    indxfrddtimeoutlsort = np.argmin(frddtimeoutlsort)\n",
    "    \n",
    "    # minimum fractional difference between differences of times of outlier samples\n",
    "    minmfrddtimeoutlsort = frddtimeoutlsort[indxfrddtimeoutlsort]\n",
    "    \n",
    "    # estimate of the epoch\n",
    "    epoccomp = timeoutl[0]\n",
    "    \n",
    "    # estimate of the period\n",
    "    pericomp = difftimeoutlsort[indxfrddtimeoutlsort]\n",
    "    \n",
    "    # output dictionary\n",
    "    dictoutp = dict()\n",
    "    \n",
    "    # populate the output dictionary\n",
    "    if minmfrddtimeoutlsort < 0.1:\n",
    "        dictoutp['boolposi'] = True\n",
    "        dictoutp['pericomp'] = pericomp\n",
    "        dictoutp['epocmtracomp'] = epoccomp\n",
    "    else:\n",
    "        dictoutp['boolposi'] = False\n",
    "    dictoutp['minmfrddtimeoutlsort'] = minmfrddtimeoutlsort\n",
    "    dictoutp['timeoutl'] = timeoutl \n",
    "    \n",
    "    return dictoutp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb93e29",
   "metadata": {},
   "source": [
    "Now we take that analysis function and use  `NestedFrame.reduce` to apply it across all of our r band subsample of ZTF lightcurves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fada3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = r_band.reduce(\n",
    "    srch_outlperi,\n",
    "    \"lc.mjd\", \n",
    "    \"lc.mag\", \n",
    "    \"lc.magerr\",\n",
    "    meta={\n",
    "        \"boolposi\": bool,\n",
    "        \"pericomp\": float,\n",
    "        \"epocmtracomp\": float,\n",
    "        \"minmfrddtimeoutlsort\": float,\n",
    "        \"timeoutl\": object,\n",
    "    },\n",
    ")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243de37",
   "metadata": {},
   "source": [
    "We can apply a Dask `compute` to materialize the lazily evaluated `outliers` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29cd6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_res = outliers.compute()\n",
    "outliers_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
