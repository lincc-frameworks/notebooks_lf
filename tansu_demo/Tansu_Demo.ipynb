{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tansu Demo\n",
    "Very rough draft that runs note that this is meant to be run on epyc or baldur\n",
    "This takes the code provided [here](https://www.google.com/url?q=https://github.com/tdaylan/miletos/blob/71f7d18542f3ef82808eba588bad6361c8297351/miletos/main.py%23L5192&sa=D&source=docs&ust=1715889313765253&usg=AOvVaw2elaC5NDhYWMIVOZa-rq90) and applies it to a subset of ztf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lsdb\n",
    "# pip install lf_tape\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import lsdb\n",
    "import tape\n",
    "\n",
    "from lsdb.core.search import BoxSearch, ConeSearch, PolygonSearch\n",
    "from tape import Ensemble, ColumnMapper\n",
    "from hipscat.io.file_io import read_parquet_metadata\n",
    "print(lsdb.__version__)\n",
    "print(tape.__version__)\n",
    "\n",
    "import dask\n",
    "dask.config.set({\"temporary-directory\" :'/epyc/ssd/users/wbeebe/tmp'})\n",
    "dask.config.set({\"dataframe.shuffle-compression\": 'Snappy'})\n",
    "dask.config.set({\"dataframe.convert-string\": False})\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dask client \n",
    "client = Client(n_workers=8, threads_per_worker=1, memory_limit='40Gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztf_object_path = \"/epyc/data3/hipscat/catalogs/ztf_axs/ztf_dr14\"\n",
    "ztf_source_path = \"/epyc/data3/hipscat/catalogs/ztf_axs/ztf_source\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztf_object = lsdb.read_hipscat(ztf_object_path, search_filter=ConeSearch(ra=-60, dec=20, radius_arcsec=1*1600))\n",
    "#sources load takes a minute, since it creates a healpix alignment on load\n",
    "ztf_source = lsdb.read_hipscat(ztf_source_path,\n",
    "                               columns=['index', 'ps1_objid',\n",
    "                                       'ra', 'dec', \n",
    "                                       'catflags', \n",
    "                                       'fieldID', \n",
    "                                       'mjd', 'band', 'mag', 'magerr', 'Npix'], search_filter=ConeSearch(ra=-60, dec=20, radius_arcsec=1*1600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztf_object_100 = ztf_object.query(\"nobs_g > 100 and nobs_r > 100\")\n",
    "# We do this to get the source catalog indexed by the objects hipscat index\n",
    "ztf_joined_source_cat = ztf_object_100.join(\n",
    "    ztf_source, left_on=\"ps1_objid\", right_on=\"ps1_objid\", suffixes=(\"_object\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colmap = ColumnMapper(\n",
    "    id_col=\"_hipscat_index\",\n",
    "    time_col=\"mjd\",\n",
    "    flux_col=\"mag\",\n",
    "    err_col=\"magerr\",  \n",
    "    band_col=\"band\",\n",
    ")\n",
    "\n",
    "ens = Ensemble(client=Client)\n",
    "\n",
    "# We just pass in the catalog objects\n",
    "ens.from_lsdb(ztf_joined_source_cat, ztf_object, column_mapper=colmap)\n",
    "\n",
    "ens.object.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a simple function\n",
    "def my_flux_average(flux_array, band_array, method=\"mean\", band=None):\n",
    "    \"\"\"Read in an array of fluxes, and return the average of the fluxes by band\"\"\"\n",
    "    if band != None:\n",
    "        mask = [band_array == band]  # Create a band by band mask\n",
    "        band_flux = flux_array[tuple(mask)]  # Mask the flux array\n",
    "        if method == \"mean\":\n",
    "            res = np.mean(band_flux)\n",
    "        elif method == \"median\":\n",
    "            res = np.median(band_flux)\n",
    "    else:\n",
    "        res = None\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to the ensemble\n",
    "res = ens.batch(my_flux_average, \"mag\", \"band\", meta=None, method=\"median\", band=\"g\")\n",
    "res_computed = res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ens.to_timeseries(3647494584189059072)  # provided a target object id\n",
    "ts.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ts_g = ts.data[ts.band == \"g\"]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.errorbar(ts_g.mjd, ts_g.mag, ts_g.magerr, fmt=\"o\", color=\"green\", alpha=0.8, label=\"g\")\n",
    "plt.xlabel(\"Time (MJD)\")\n",
    "plt.ylabel(\"Flux (mJy)\")\n",
    "plt.minorticks_on()\n",
    "plt.legend(title=\"Band\", loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srch_outlperi( \\\n",
    "                  # time of samples\n",
    "                  time, \\\n",
    "                  # relative flux of samples\n",
    "                  flux, \\\n",
    "                  # relative flux error of samples\n",
    "                  stdvflux, \\\n",
    "                  # number of outliers to include in the search\n",
    "                  numboutl=5, \\\n",
    "                  # Boolean flag to diagnose\n",
    "                  booldiag=True, \\\n",
    "                 ):\n",
    "    '''\n",
    "    Search for periodic outliers in a computationally efficient way\n",
    "    '''\n",
    "    \n",
    "    # indices of the outliers\n",
    "    indxtimesort = np.argsort(flux)[::-1][:numboutl]\n",
    "    \n",
    "    # the times of the outliers\n",
    "    timeoutl = time[indxtimesort]\n",
    "    \n",
    "    # number of differences between times of outlier samples\n",
    "    numbdiff = int(numboutl * (numboutl - 1) / 2)\n",
    "    \n",
    "    # differences between times of outlier samples\n",
    "    difftimeoutl = np.empty(numbdiff)\n",
    "    \n",
    "    # compute the differences between times of outlier samples\n",
    "    listtemp = []\n",
    "    c = 0\n",
    "    indxoutl = np.arange(numboutl)\n",
    "    for a in indxoutl:\n",
    "        for b in indxoutl:\n",
    "            if a >= b:\n",
    "                continue\n",
    "            listtemp.append([a, b])\n",
    "            difftimeoutl[c] = abs(timeoutl[a] - timeoutl[b])\n",
    "            c += 1\n",
    "    \n",
    "    # incides that sort the differences between times of outlier samples\n",
    "    indxsort = np.argsort(difftimeoutl)\n",
    "    \n",
    "    # sorted differences between times of outlier samples\n",
    "    difftimeoutlsort = difftimeoutl[indxsort]\n",
    "\n",
    "    # fractional differences between differences of times of outlier samples\n",
    "    frddtimeoutlsort = (difftimeoutlsort[1:] - difftimeoutlsort[:-1]) / ((difftimeoutlsort[1:] + difftimeoutlsort[:-1]) / 2.)\n",
    "\n",
    "    # index of the minimum fractional difference between differences of times of outlier samples\n",
    "    indxfrddtimeoutlsort = np.argmin(frddtimeoutlsort)\n",
    "    \n",
    "    # minimum fractional difference between differences of times of outlier samples\n",
    "    minmfrddtimeoutlsort = frddtimeoutlsort[indxfrddtimeoutlsort]\n",
    "    \n",
    "    # estimate of the epoch\n",
    "    epoccomp = timeoutl[0]\n",
    "    \n",
    "    # estimate of the period\n",
    "    pericomp = difftimeoutlsort[indxfrddtimeoutlsort]\n",
    "    \n",
    "    # output dictionary\n",
    "    dictoutp = dict()\n",
    "    \n",
    "    # populate the output dictionary\n",
    "    if minmfrddtimeoutlsort < 0.1:\n",
    "        dictoutp['boolposi'] = True\n",
    "        dictoutp['pericomp'] = [pericomp]\n",
    "        dictoutp['epocmtracomp'] = [epoccomp]\n",
    "    else:\n",
    "        dictoutp['boolposi'] = False\n",
    "    dictoutp['minmfrddtimeoutlsort'] = [minmfrddtimeoutlsort]\n",
    "    dictoutp['timeoutl'] = timeoutl \n",
    "    \n",
    "    return dictoutp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ens.batch(\n",
    "    srch_outlperi,\n",
    "    ens._time_col,\n",
    "    ens._flux_col,\n",
    "    ens._err_col, # TODO should be std dev of flux error? Is this right?\n",
    "    meta=None)\n",
    "res_computed = res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tape_static",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
