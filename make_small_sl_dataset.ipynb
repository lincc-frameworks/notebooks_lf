{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c67aa3",
   "metadata": {},
   "source": [
    "# Strong Lensing Challenge data preparation for Hyrax\n",
    "## Goals of this work\n",
    "* Produce a small version of the Strong Lensing Challenge dataset that contains only N samples from each class.\n",
    "* Produce a `combined_slsim` and `combined_hsc` dataset where each are a single directory containing equal numbers of lens and non-lens samples of either simulated or hsc data.\n",
    "\n",
    "## Prerequisites\n",
    "* You have downloaded all the training data available for the SL Challenge https://slchallenge.cbpf.br/ \n",
    "  * 4 zip files; 1. hsc_lenses 2. hsc_nonlenses 3. slsim_lenses 4. slsim_nonlenses\n",
    "* You have extracted all the training data in to subdirectories under a single data directory i.e. `/home/user/sl_data/`\n",
    "* IMPORTANT - You have moved the corresponding `parameters.fits` files into it's appropriate subdirectory.\n",
    "  * Note - When unzipped the `parameters.fits` file will be outside the folder containing the .fits image files.\n",
    "\n",
    "## Notes about methodology\n",
    "When creating the smaller dataset classes, it's tempting to simply take the first N .fits images from the original dataset.\n",
    "Unfortunately the `parameters.fits` files don't always contain a row for a given object or lens id.\n",
    "Therefore, we take the following approach, roughly defined in pseudocode:\n",
    "0) Loop over .fits files by index `i`\n",
    "1) Select and open .fits image file `i`\n",
    "2) Set `obj_id` equal to the object id in the file\n",
    "3) Find `obj_id` in the `parameters.fits` table\n",
    "4) If the `obj_id` is in the table\n",
    "   1) Append `obj_id` to the list `ids`\n",
    "   2) Copy the set of .fits files to th smaller data directory\n",
    "   3) Rename the .fits files to have `index = length(ids)`\n",
    "5) Stop `len(ids)` is equal to the desired size\n",
    "6) Use `ids` to index into `parameters.fits` table to select a subset of rows\n",
    "7) Write out the subset of rows as a new `parameter.fits` file in the smaller data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05bead9",
   "metadata": {},
   "source": [
    "## Set up the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5970c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from astropy.table import Table, vstack\n",
    "from astropy.io import fits\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53cf7dd",
   "metadata": {},
   "source": [
    "## Edit these parameters as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7688cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of samples from each of the 4 classes of data. i.e. dataset_size = 10 will result in 40 objects each with 5 bands, meaning 200 .fits files total\n",
    "smaller_dataset_size = 100\n",
    "\n",
    "# The parent directory that contains the smaller dataset. i.e. where you want to save the smaller dataset\n",
    "smaller_data_directory = Path(f\"/Users/drew/sl_data_challenge/sl_{smaller_dataset_size}\")\n",
    "\n",
    "# The directory containing the uncompressed original data\n",
    "original_data_directory = Path(\"/Users/drew/sl_data_challenge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5b5bb",
   "metadata": {},
   "source": [
    "### Function to copy subset of original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original datasets each contain this many objects (5 bands each, so 250_000 total .fits files)\n",
    "original_dataset_size = 50_000  #! Be careful editing this number\n",
    "\n",
    "def create_smaller_sample(subset, smaller_dataset_size=smaller_dataset_size):\n",
    "    # directory containing the original data\n",
    "    root_dir = original_data_directory / subset\n",
    "\n",
    "    # directory where the smaller dataset will be saved\n",
    "    output_dir = smaller_data_directory / subset\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Read in the original parameters table\n",
    "    params_table = Table.read(root_dir / \"parameters.fits\")\n",
    "    if \"Lens ID\" in params_table.columns:\n",
    "        id_column_name = \"Lens ID\"\n",
    "    else:\n",
    "        id_column_name = \"Object ID\"\n",
    "\n",
    "    ids = []\n",
    "    i = 0\n",
    "    with tqdm(total=smaller_dataset_size) as pbar:\n",
    "\n",
    "        # Work through the original files until we have accumulated the number we want, or we run out of files\n",
    "        while len(ids) < smaller_dataset_size and i < original_dataset_size:\n",
    "            # glob pattern to get the file set for the i_th index\n",
    "            pattern = f\"*_{str(i).zfill(8)}_*.fits\"\n",
    "            files = list(root_dir.glob(pattern))\n",
    "\n",
    "            # if there are not 5 files, move to the next file set\n",
    "            if len(files) != 5:\n",
    "                print(f\"\\tExpected 5 files, found {len(files)} for pattern {pattern}\")\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # open one of the files\n",
    "            raw_data = fits.getdata(files[0], 1)\n",
    "\n",
    "            # get the object id as a string from raw_data\n",
    "            object_id = str(raw_data[0][0])\n",
    "\n",
    "            # if the object id is in params table...\n",
    "            if np.any(np.isin(params_table[id_column_name].astype(str), object_id)):\n",
    "                # copy the 5 files over, and update their indexes to match.\n",
    "                for file in files:\n",
    "                    # replace a portion of the filename with the current index\n",
    "                    new_filename = file.name.replace(\n",
    "                        f\"_{str(i).zfill(8)}_\",\n",
    "                        f\"_{str(len(ids)).zfill(8)}_\"\n",
    "                    )\n",
    "                    shutil.copy(file, output_dir / new_filename)\n",
    "\n",
    "                # Add the object id to the list\n",
    "                ids.append(object_id)\n",
    "\n",
    "                # update the progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "            # move to the next file set\n",
    "            i += 1\n",
    "\n",
    "    # Filter the original parameters table to only include rows for the files that were copied\n",
    "    small_params_table = params_table[np.isin(params_table[id_column_name].astype(str), ids)]\n",
    "\n",
    "    # Write out the new smaller parameters file.\n",
    "    small_params_table.write(output_dir / \"parameters.fits\", format='fits', overwrite=True)\n",
    "\n",
    "    return small_params_table, output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The allowed file_prefixes are:\n",
    "\"D1_L\" - slsim lenses\n",
    "\"D1_N\" - slsim non-lenses\n",
    "\"D2_L\" - HSC lenses\n",
    "\"D2_N\" - HSC non-lenses\n",
    "\"\"\"\n",
    "allowed_file_prefixes = [\"D1_L\", \"D1_N\", \"D2_L\", \"D2_N\"]\n",
    "\n",
    "def verify_file_and_table_order(output_dir, file_prefix=\"D2_L\"):\n",
    "    if file_prefix not in allowed_file_prefixes:\n",
    "        raise ValueError(f\"file_prefix must be one of {allowed_file_prefixes}\")\n",
    "\n",
    "    # Open the newly created small parameters table\n",
    "    small_table = Table.read(output_dir / \"parameters.fits\")\n",
    "    if \"Lens ID\" in small_table.columns:\n",
    "        id_column_name = \"Lens ID\"\n",
    "    else:\n",
    "        id_column_name = \"Object ID\"\n",
    "\n",
    "    # For each row in the table\n",
    "    for i in tqdm(range(len(small_table))):\n",
    "        # Open one of the corresponding files\n",
    "        raw_data = fits.getdata(output_dir / f\"{file_prefix}_{str(i).zfill(8)}_g.fits\")\n",
    "        # Compare the object id in the file to the object id in the table\n",
    "        table_id = small_table[i][id_column_name]\n",
    "        if not isinstance(table_id, str):\n",
    "            table_id = str(table_id)\n",
    "        if not table_id == str(raw_data[0][0]):\n",
    "            print(\"Problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ff60d",
   "metadata": {},
   "source": [
    "## HSC Lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62afee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = \"hsc_lenses\"\n",
    "smaller_params_table, output_dir = create_smaller_sample(subset)\n",
    "print(\"Completed creating smaller sample.\")\n",
    "verify_file_and_table_order(output_dir, file_prefix=\"D2_L\")\n",
    "print(\"Finished verification test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037999d",
   "metadata": {},
   "source": [
    "## HSC Non-Lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba45dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = \"hsc_nonlenses\"\n",
    "smaller_params_table, output_dir = create_smaller_sample(subset)\n",
    "print(\"Completed creating smaller sample.\")\n",
    "verify_file_and_table_order(output_dir, file_prefix=\"D2_N\")\n",
    "print(\"Finished verification test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797e273",
   "metadata": {},
   "source": [
    "## Simulated Lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b731ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = \"slsim_lenses\"\n",
    "smaller_params_table, output_dir = create_smaller_sample(subset)\n",
    "print(\"Completed creating smaller sample.\")\n",
    "verify_file_and_table_order(output_dir, file_prefix=\"D1_L\")\n",
    "print(\"Finished verification test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377bb39",
   "metadata": {},
   "source": [
    "## Simulated Non-Lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = \"slsim_nonlenses\"\n",
    "smaller_params_table, output_dir = create_smaller_sample(subset)\n",
    "print(\"Completed creating smaller sample.\")\n",
    "verify_file_and_table_order(output_dir, file_prefix=\"D1_N\")\n",
    "print(\"Finished verification test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07b97f",
   "metadata": {},
   "source": [
    "# Make combined datasets for training on both lens and non-lens images\n",
    "The method for creating the combined datasets is identical for both slsim and HSC data.\n",
    "1) Open both the parameters.fits tables, concatenate them together and write that out.\n",
    "2) Create symlinks for all of the lens .fits files in the new combined directory\n",
    "3) Create symlinks for all of the non-lens .fits files, but update the index to be `i+N`\n",
    "   * `i` = the original index. i.e. `00000012`.\n",
    "   * `N` = the number of objects in the smaller dataset. i.e. `100`\n",
    "   * This update means that the filename index will match the row in the `parameters.fits` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5442a",
   "metadata": {},
   "source": [
    "### Create combined SLSIM dataset\n",
    "This contains N samples of lens and non-lens objects. There will be 5 .fits files per object.\n",
    "The naming scheme is the same as for the original dataset.\n",
    "The `parameters.fits` is simply the concatenation of the two original `parameters.fits` files, therefore there is almost no column overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7baa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined parameters file that just contains object ids\n",
    "\n",
    "small_lenses_dir = smaller_data_directory / \"slsim_lenses\"\n",
    "slsim_lenses_table = Table.read(small_lenses_dir / \"parameters.fits\")\n",
    "\n",
    "small_nonlenses_dir = smaller_data_directory / \"slsim_nonlenses\"\n",
    "slsim_nonlenses_table = Table.read(small_nonlenses_dir / \"parameters.fits\")\n",
    "\n",
    "combined_dir = smaller_data_directory / \"slsim_combined\"\n",
    "combined_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Concatenate the lens and non-lens parameters files together.\n",
    "combined = vstack([slsim_lenses_table, slsim_nonlenses_table])\n",
    "combined.write(combined_dir / \"parameters.fits\", format='fits', overwrite=True)\n",
    "\n",
    "# Create symlinks in the combined directory for the lens files\n",
    "for i in range(smaller_dataset_size):\n",
    "    pattern = f\"*L*_{str(i).zfill(8)}_*.fits\"\n",
    "    files = list(small_lenses_dir.glob(pattern))\n",
    "\n",
    "    for src in files:\n",
    "        dst = combined_dir / src.name\n",
    "        if os.path.islink(dst):\n",
    "            os.unlink(dst)\n",
    "        os.symlink(src, dst)\n",
    "\n",
    "# Create symlinks in the combined directory for the non-lens files\n",
    "for i in range(smaller_dataset_size):\n",
    "    pattern = f\"*N*_{str(i).zfill(8)}_*.fits\"\n",
    "    files = list(small_nonlenses_dir.glob(pattern))\n",
    "\n",
    "    for src in files:\n",
    "        # increment file indexes by the total number of lens files\n",
    "        # e.g. D1_N_00000010_g.fits becomes D1_N_00000110_g.fits\n",
    "        new_index = i + (1 * smaller_dataset_size)\n",
    "        new_filename = src.name.replace(\n",
    "                        f\"_{str(i).zfill(8)}_\",\n",
    "                        f\"_{str(new_index).zfill(8)}_\"\n",
    "                    )\n",
    "        dst = combined_dir / new_filename\n",
    "        if os.path.islink(dst):\n",
    "            os.unlink(dst)\n",
    "        os.symlink(src, dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f26c0",
   "metadata": {},
   "source": [
    "### Create combined HSC dataset\n",
    "This contains N samples of lens and non-lens objects. There will be 5 .fits files per object.\n",
    "The naming scheme is the same as for the original dataset.\n",
    "The `parameters.fits` is simply the concatenation of the two original `parameters.fits` files, therefore there is almost no column overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined parameters file that just contains object ids\n",
    "\n",
    "small_lenses_dir = smaller_data_directory / \"hsc_lenses\"\n",
    "hsc_lenses_table = Table.read(small_lenses_dir / \"parameters.fits\")\n",
    "\n",
    "small_nonlenses_dir = smaller_data_directory / \"hsc_nonlenses\"\n",
    "hsc_nonlenses_table = Table.read(small_nonlenses_dir / \"parameters.fits\")\n",
    "\n",
    "combined_dir = smaller_data_directory / \"hsc_combined\"\n",
    "combined_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Concatenate the lens and non-lens parameters files together.\n",
    "combined = vstack([hsc_lenses_table, hsc_nonlenses_table])\n",
    "combined.write(combined_dir / \"parameters.fits\", format='fits', overwrite=True)\n",
    "\n",
    "# Create symlinks in the combined directory for the lens files\n",
    "for i in range(smaller_dataset_size):\n",
    "    pattern = f\"*L*_{str(i).zfill(8)}_*.fits\"\n",
    "    files = list(small_lenses_dir.glob(pattern))\n",
    "\n",
    "    for src in files:\n",
    "        dst = combined_dir / src.name\n",
    "        if os.path.islink(dst):\n",
    "            os.unlink(dst)\n",
    "        os.symlink(src, dst)\n",
    "\n",
    "# Create symlinks in the combined directory for the non-lens files\n",
    "for i in range(smaller_dataset_size):\n",
    "    pattern = f\"*N*_{str(i).zfill(8)}_*.fits\"\n",
    "    files = list(small_nonlenses_dir.glob(pattern))\n",
    "\n",
    "    for src in files:\n",
    "        # increment file indexes by the total number of lens files\n",
    "        # e.g. D1_N_00000010_g.fits becomes D1_N_00000110_g.fits\n",
    "        new_index = i + (1 * smaller_dataset_size)\n",
    "        new_filename = src.name.replace(\n",
    "                        f\"_{str(i).zfill(8)}_\",\n",
    "                        f\"_{str(new_index).zfill(8)}_\"\n",
    "                    )\n",
    "        dst = combined_dir / new_filename\n",
    "        if os.path.islink(dst):\n",
    "            os.unlink(dst)\n",
    "        os.symlink(src, dst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyrax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
