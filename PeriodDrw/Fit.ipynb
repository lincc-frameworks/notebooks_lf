{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Neven Caplar \n",
    "Last updated: 2023-10-07\n",
    "\n",
    "Goals: \n",
    "Fit the data\n",
    "\n",
    "Each Section can/should run independently,\n",
    "only these initial imports should be shared among all sections\n",
    "\n",
    "Questions:\n",
    "What determines memory limit of the workers\n",
    "How to partition the dataframe in order to get more workers active\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# from scipy.spatial import KDTree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import JaxPeriodDrwFit\n",
    "\n",
    "\n",
    "from tape.ensemble import Ensemble\n",
    "from tape.utils import ColumnMapper\n",
    "\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "# many workers\n",
    "# dask.config.set(scheduler='threads') \n",
    "\n",
    "# does not work\n",
    "# from multiprocessing.pool import ThreadPool\n",
    "# dask.config.set(pool=ThreadPool(20))\n",
    "\n",
    "# one worker\n",
    "# dask.config.set(scheduler='processes')  \n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "# cluster.adapt(minimum=10, maximum=40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = Ensemble(client = client)  # initialize an ensemble object\n",
    "ens.client_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup base directory for saving output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username= \"wbeebe\"\n",
    "basedir = f\"/astro/users/{username}/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tape Single Pixel - real data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running on baldur\n",
    "data_path = \"/astro/store/epyc/data3/hipscat/catalogs/tape_test/\"\n",
    "\n",
    "col_map = ColumnMapper(id_col=\"SDSS_NAME_dr16q_constant\", \n",
    "                       time_col=\"mjd_ztf_source\",\n",
    "                       flux_col=\"mag_ztf_source\", \n",
    "                       err_col=\"magerr_ztf_source\",\n",
    "                       band_col=\"band_ztf_source\")\n",
    "\n",
    "ens.from_hipscat(data_path,\n",
    "                 source_subdir=\"tape_test_sources\",\n",
    "                 object_subdir=\"tape_test_obj\",\n",
    "                 column_mapper=col_map,\n",
    "                 additional_cols=True,\n",
    "                 sync_tables=True,\n",
    "                 npartitions=10\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.query(\"band_ztf_source == 'g'\", table = 'source')\n",
    "ens.prune(10)\n",
    "ens.query(\"rMeanPSFMag_ps1_otmo < 20\", table = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9min, 13 sec on baldur, for 603 sources in 4 partitions\n",
    "# 7min, 30 sec on baldur, for 603 sources in 4 partitions, Nov 7\n",
    "# 5min, 14 sec on baldur, for 603 sources in 10 partitions, Nov 7\n",
    "# 3min, 3 sec on baldur, with padding \n",
    "JaxPeriodDrwFit_instance = JaxPeriodDrwFit.JaxPeriodDrwFit()\n",
    "res_tsp = ens.batch(JaxPeriodDrwFit_instance.optimize_map, 'mjd_ztf_source', \"mag_ztf_source\", \"magerr_ztf_source\",\n",
    "                compute=True, meta=None, n_init=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JaxPeriodDrwFit_instance = JaxPeriodDrwFit.JaxPeriodDrwFit()\n",
    "res_tsp_drw = ens.batch(JaxPeriodDrwFit_instance.optimize_map_drw, 'mjd_ztf_source', \"mag_ztf_source\", \"magerr_ztf_source\",\n",
    "                compute=True, meta=None, n_init=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username=\"wbeebe\"\n",
    "def pack_output_to_parquet(result, cols, output_dir, output_filename, drop_cols=[], full=False):\n",
    "    \"\"\"Packs output to a dataframe, written as a parquet file. The created dataframe object is returned for inspection.\"\"\"\n",
    "    result_df = None\n",
    "    if full:\n",
    "        # Construct dataframes with the results for each object.\n",
    "        dfs = []\n",
    "        for i in range(len(result)):\n",
    "            obj_data = result.iloc[i]\n",
    "            # Construct a series representing the index\n",
    "            obj_index = pd.Series(np.full(len(obj_data), result.index[i]), name=result.index.name)\n",
    "            dfs.append(pd.DataFrame(data=obj_data, columns=cols, index=obj_index))\n",
    "\n",
    "        # Concatenate all of the per-object dataframes\n",
    "        result_df = pd.concat(dfs)\n",
    "    else:\n",
    "        # Each object only has a 1D array in the result series, so the constructed\n",
    "        # dataframe has the same number of rows. So we can just do a 1:1 mapping with column names \n",
    "        result_df = pd.DataFrame(columns=cols, index=result.index)\n",
    "        for i in range(len(result)):\n",
    "            result_df.iloc[i] = result[i]\n",
    "\n",
    "    # Drop any columns if requested.\n",
    "    if drop_cols:\n",
    "        result_df = result_df.drop(columns=drop_cols)\n",
    "\n",
    "    # Write the output to the parquet file\n",
    "    pa_table = pa.Table.from_pandas(result_df)\n",
    "    pa_table\n",
    "    pa.parquet.write_table(pa_table, f\"{output_dir}/data/{output_filename}.parquet\")\n",
    "    return result_df\n",
    "\n",
    "# Create columns for result of using just the drw kernel\n",
    "param_cols = ['log_drw_scale', 'log_drw_amp']\n",
    "init_param_cols = [\"init_\" + c for c in param_cols]\n",
    "drw_columns = ['min_neg_log_lh', 'neg_log_lh'] + param_cols + init_param_cols\n",
    "\n",
    "# Create columns for result of combining the drw params with periodic params\n",
    "param_cols = ['log_drw_scale', 'log_drw_amp', 'log_per_scale', 'log_per_amp']\n",
    "init_param_cols = [\"init_\" + c for c in param_cols]\n",
    "combined_columns = ['min_neg_log_lh', 'neg_log_lh'] + param_cols + init_param_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output for results from just the drw kernel\n",
    "drw_df = pack_output_to_parquet(res_tsp_drw, drw_columns,\n",
    "                       f\"/astro/users/{username}\", \"res_tsp_run_g_0_drw\")\n",
    "drw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output for results from the combined drw and periodic kernel\n",
    "combined_df = pack_output_to_parquet(res_tsp, combined_columns,\n",
    "                       f\"/astro/users/{username}\", \"res_tsp_run_g_0\")\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redo but save all results (full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JaxPeriodDrwFit_instance = JaxPeriodDrwFit.JaxPeriodDrwFit()\n",
    "res_tsp_full = ens.batch(JaxPeriodDrwFit_instance.optimize_map, 'mjd_ztf_source', \"mag_ztf_source\", \"magerr_ztf_source\",\n",
    "                compute=True, meta=None, n_init=100, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JaxPeriodDrwFit_instance = JaxPeriodDrwFit.JaxPeriodDrwFit()\n",
    "res_tsp_drw_full = ens.batch(JaxPeriodDrwFit_instance.optimize_map_drw, 'mjd_ztf_source', \"mag_ztf_source\", \"magerr_ztf_source\",\n",
    "                compute=True, meta=None, n_init=100, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output for results from just the drw kernel\n",
    "drw_df_full = pack_output_to_parquet(res_tsp_drw_full, drw_columns,\n",
    "                       f\"/astro/users/{username}\", \"res_tsp_run_g_0_drw_full\", full=True)\n",
    "drw_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output for results from the combined drw and periodic kernel\n",
    "combined_df_full = pack_output_to_parquet(res_tsp_full, combined_columns,\n",
    "                       f\"/astro/users/{username}\", \"res_tsp_run_g_0_full\", full=True)\n",
    "combined_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lf_tape_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
